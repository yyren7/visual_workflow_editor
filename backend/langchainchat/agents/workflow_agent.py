from typing import Dict, Any, List, Optional, Type
import logging # Import logging

from langchain_core.runnables import Runnable, RunnableConfig, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Removed PromptTemplate import if not needed elsewhere
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import StructuredTool, BaseTool # Import BaseTool as well
from langchain.agents import AgentExecutor, create_structured_chat_agent # Import AgentExecutor and constructor
# Removed unused format_tools_for_llm and parse_llm_output placeholders
# from langchain.agents.format_scratchpad.structured_chat import format_to_structured_chat_scratchpad # Might not be needed directly

from backend.langchainchat.tools.executor import ToolExecutor

# 根据官方文档，直接从 langchain_deepseek 导入 ChatDeepSeek (注意大小写)
from langchain_deepseek import ChatDeepSeek 
# Import Gemini class (ensure you have 'langchain-google-genai' installed)
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    # Define a placeholder if not installed, so the code doesn't break
    # but clearly indicates Gemini support is missing.
    ChatGoogleGenerativeAI = Type['ChatGoogleGenerativeAI'] # type: ignore
    logger.warning("langchain-google-genai not installed. Gemini agent support will be unavailable.")

logger = logging.getLogger(__name__) # Setup logger

# --- Internal function specifically for DeepSeek/Structured Chat Agent --- 
def _create_deepseek_structured_agent_runnable(llm: BaseChatModel, tools: List[BaseTool]) -> Runnable:
    """
    Internal helper to create the AgentExecutor using the structured chat approach,
    suitable for models like DeepSeek that benefit from explicit JSON instructions.
    """
    tool_names = ", ".join([t.name for t in tools])

    system_prompt_template = """Respond to the human as helpfully as possible. 
You have access to the following tools:

{tools}

Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).

Valid "action" values: "final_answer" or [{tool_names}]

Provide only ONE action per $JSON_BLOB, as shown:

```json
{{
  "action": $TOOL_NAME,
  "action_input": $INPUT
}}
```

Should you decide to respond with a final answer, output ONLY a single json blob, as shown below:

```json
{{
  "action": "final_answer",
  "action_input": "Your final response to the human goes here."
}}
```
Remember to ALWAYS use the exact tool names provided.

CURRENT WORKFLOW CONTEXT:
{flow_context}
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt_template),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    prompt = prompt.partial(tool_names=tool_names)
    logger.debug(f"DeepSeek Structured Prompt (partial): {prompt}")

    agent = create_structured_chat_agent(llm, tools, prompt)
    logger.debug(f"Created structured chat agent for DeepSeek: {agent}")

    agent_executor = AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors="Check your output and make sure it conforms to the JSON format instructions in the system prompt!",
        max_iterations=10,
    )
    return agent_executor

# --- Factory function to create the appropriate agent based on LLM type --- 
def create_workflow_agent_runnable(llm: BaseChatModel, tool_executor: ToolExecutor) -> Runnable:
    """
    Factory function to create the appropriate workflow agent runnable 
    based on the provided LLM type.
    """
    tools: List[BaseTool] = tool_executor.get_langchain_tools()
    if not tools:
        logger.warning("No Langchain tools were generated by ToolExecutor. Agent might not function correctly.")

    # --- Select Agent Strategy based on LLM type --- 
    
    # Example: Check for Gemini first (requires langchain-google-genai)
    if isinstance(llm, ChatGoogleGenerativeAI):
        logger.info("Detected Gemini model. Creating Gemini-specific agent (Not Implemented Yet).")
        # TODO: Implement agent creation logic suitable for Gemini's Tool Calling
        # This might involve a different prompt and potentially a different agent constructor 
        # like create_openai_tools_agent or similar, adapted for Gemini.
        # Example placeholder:
        # from langchain.agents import create_openai_tools_agent
        # gemini_prompt = ChatPromptTemplate.from_messages([...]) # Simpler prompt for tool calling
        # gemini_agent = create_openai_tools_agent(llm, tools, gemini_prompt)
        # agent_executor = AgentExecutor(agent=gemini_agent, tools=tools, verbose=True, ...)
        # return agent_executor
        
        # For now, raise an error or fall back to the default
        # Option 1: Raise Error
        # raise NotImplementedError("Agent creation for Gemini models is not yet implemented.")
        
        # Option 2: Fallback to structured agent (might work, might not be optimal)
        logger.warning("Falling back to structured chat agent for Gemini. This might not be optimal.")
        agent_executor = _create_deepseek_structured_agent_runnable(llm, tools)
        
    # Assume DeepSeek or other models requiring structured chat approach
    # (You could add more specific checks like isinstance(llm, ChatDeepSeek))
    else:
        logger.info(f"Detected model type: {type(llm).__name__}. Creating DeepSeek/Structured Chat agent.")
        agent_executor = _create_deepseek_structured_agent_runnable(llm, tools)

    # --- Return the configured AgentExecutor --- 
    logger.info(f"Created AgentExecutor with {len(tools)} tools for {type(llm).__name__}.")
    return agent_executor

# Example Input structure expected by the AgentExecutor call:
# {
#     "input": "User's message",
#     "chat_history": [HumanMessage(...), AIMessage(...)],
#     "flow_context": {"nodes": [...], "edges": [...]}
# } 