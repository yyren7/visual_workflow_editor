## backend/app/main.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import JSONResponse # Not used directly in provided snippet, keep if used elsewhere
import sys
import os
import logging # Keep for getting logger instances
from pathlib import Path
# import logging.handlers # No longer directly used here
import time # ÂØºÂÖ• time
import uuid # ADDED
import datetime # ADDED
from typing import Optional # ADDED
from fastapi import HTTPException # ADDED
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver # ADDED
from backend.config import DB_CONFIG # ADDED - Assuming DB_CONFIG is here or accessible
from contextlib import asynccontextmanager
import asyncio
from datetime import datetime, timedelta
from typing import AsyncGenerator

# --- REMOVE OLD LangChain debug ---
# import langchain
# langchain.debug = True
# logger = logging.getLogger(__name__)
# logger.info("LangChain debug mode enabled.")
# --- END REMOVE ---

# Ê∑ªÂä†È°πÁõÆÊ†πÁõÆÂΩïÂà∞PythonË∑ØÂæÑ (Keep this)
BASE_DIR = Path(__file__).resolve().parent.parent.parent
sys.path.append(str(BASE_DIR))

# --- NEW: Import and call centralized logging configuration ---
from backend.logging_config import setup_app_logging
setup_app_logging() # Configure logging for the entire backend
# --- END NEW ---

# Get a logger instance for this module (backend.app.main)
# This logger will inherit configuration from the 'backend' logger setup in logging_config
logger = logging.getLogger(__name__)

# Imports for Checkpointer
from typing import Optional # ADDED
from fastapi import HTTPException, FastAPI as FastAPIInstance # MODIFIED: Added FastAPIInstance for app.state type hint if needed below
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver # ADDED
from backend.config import DB_CONFIG # ADDED - Assuming DB_CONFIG is here or accessible

# No longer need global CHECKPOINTER_INSTANCE if app.state is used consistently
# CHECKPOINTER_INSTANCE: Optional[AsyncPostgresSaver] = None

# ÂØºÂÖ•ÈÖçÁΩÆ (Keep this)
from backend.config.base import LOG_DIR # LOG_DIR is now primarily for reference if needed
from backend.config.app_config import APP_CONFIG, is_cors_origin_allowed

# ÂàõÂª∫logsÁõÆÂΩï (This is now handled by logging_config.py or base.py, can be removed or kept for explicitness if preferred)
# log_dir = Path(LOG_DIR)
# log_dir.mkdir(parents=True, exist_ok=True)

# --- REMOVE OLDÂàÜÊï£ÁöÑÊó•ÂøóÈÖçÁΩÆ ---
# # ‰∏∫ app.main logger ÂàõÂª∫Ê†ºÂºèÂåñÂô®
# app_main_formatter = logging.Formatter(
#     '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# # ÈÖçÁΩÆ \'backend.app\' ÂëΩÂêçÁ©∫Èó¥‰∏ãÁöÑÊó•ÂøóËÆ∞ÂΩïÂô®...
# backend_app_logger = logging.getLogger("backend.app")
# backend_app_logger.setLevel(logging.DEBUG)
# general_console_handler = logging.StreamHandler()
# general_console_handler.setFormatter(app_main_formatter)
# general_console_handler.setLevel(logging.DEBUG)
# backend_app_logger.addHandler(general_console_handler)
# backend_app_logger.propagate = False
# # ÂàõÂª∫ÂçïÁã¨ÁöÑDeepSeekÊó•ÂøóËÆ∞ÂΩïÂô®
# deepseek_logger = logging.getLogger("backend.deepseek")
# # ... (all old deepseek_logger, workflow_logger, app_main logger (__name__), sas_logger configurations) ...
# logger.propagate = False # Èò≤Ê≠¢ app.main Êó•ÂøóË¢´Ê†πËÆ∞ÂΩïÂô®ÔºàÂ¶ÇÊûúÂ∞ÜÊù•ÈÖçÁΩÆ‰∫ÜÔºâÈáçÂ§çÂ§ÑÁêÜ
# # +++ BEGINN ADDITION FOR SAS LOGGER CONFIGURATION +++
# # ... (sas_logger configuration) ...
# # +++ END ADDITION FOR SAS LOGGER CONFIGURATION +++
# logger.info("Êó•ÂøóÁ≥ªÁªüÂ∑≤ÈÖçÁΩÆ (app.main)ÔºåÂ∞ÜËÆ∞ÂΩïÂà∞ %s ÂíåÊéßÂà∂Âè∞", log_dir)
# --- END REMOVE OLD ---

logger.info("Logging system configured via backend.logging_config.py.") # New log message

# Ê£ÄÊü•ÊòØÂê¶‰ΩøÁî®ÊúÄÂ∞èÊ®°Âºè (Keep this section)
MINIMAL_MODE = os.environ.get("SKIP_COMPLEX_ROUTERS", "0") == "1"
if MINIMAL_MODE:
    logger.info("Using minimal mode, skipping some complex routers.")

# Á°Æ‰øù base Ê®°ÂùóÁöÑ LOG_DIR Â∑≤ÁªèÂä†ËΩΩ (Keep for verification if needed)
logger.info(f"LOG_DIR from config: {LOG_DIR}")

logger.info("Importing modules...") # Keep

# È¶ñÂÖàÂØºÂÖ•Êï∞ÊçÆÂ∫ìÊ®°Âûã (Keep this section)
try:
    from database.connection import Base
    logger.info("Successfully imported database.connection.Base")
    from database.models import User, Flow, FlowVariable, VersionInfo, Chat
    logger.info("Successfully imported database.models")
    from backend.config import APP_CONFIG # Keep, though imported earlier too
    logger.info("Successfully imported backend.config.APP_CONFIG")
    from backend.app.routers import (
        user, flow, email, auth, node_templates,
        flow_variables, chat, sas_chat
    )
    logger.info("Successfully imported basic routers.")
    if not MINIMAL_MODE:
        try:
            # from backend.app.routers import workflow_router # Example, keep if used
            logger.info("Successfully imported workflow_router (if applicable).")
        except ImportError as e:
            logger.error(f"Failed to import workflow_router: {e}")
    from backend.app.utils import get_version, get_version_info
    logger.info("Successfully imported backend.app.utils.")
    from backend.app.dependencies import get_node_template_service
    logger.info("Successfully imported backend.app.dependencies.")
except Exception as e:
    logger.error(f"Error during module imports: {e}", exc_info=True)
    raise

# --- Êñ∞Â¢ûÔºöÂØºÂÖ• Pydantic Ê®°ÂûãÂíå‰æùËµñ --- (Keep if relevant)
from backend.langgraphchat.memory.db_chat_memory import DbChatMemory
from backend.app.services.chat_service import ChatService

# --- Êñ∞Â¢ûÔºöËß£Êûê Pydantic ÂâçÂêëÂºïÁî® --- (Keep if relevant)
try:
    logger.info("Rebuilding Pydantic models to resolve forward references...")
    DbChatMemory.model_rebuild()
    # ChatService.model_rebuild() # If needed
    logger.info("Pydantic models rebuilt successfully.")
except Exception as e:
    logger.error(f"Error rebuilding Pydantic models: {e}", exc_info=True)

logger.info("Initializing FastAPI application...") # Keep

# Êñ∞Â¢ûÔºöË∂ÖÊó∂Ê£ÄÊµãÂíåÊÅ¢Â§ç‰ªªÂä°
async def stuck_state_monitor_task():
    """
    ÂêéÂè∞‰ªªÂä°ÔºöÂÆöÊúüÊ£ÄÊü•Âç°‰ΩèÁöÑÂ§ÑÁêÜÁä∂ÊÄÅÂπ∂Â∞ùËØïÊÅ¢Â§ç
    """
    stuck_monitor_logger = logging.getLogger("backend.app.stuck_monitor")
    
    while True:
        try:
            await asyncio.sleep(300)  # ÊØè5ÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°
            
            if not hasattr(app.state, 'checkpointer_instance') or app.state.checkpointer_instance is None:
                continue
                
            stuck_monitor_logger.info("Starting stuck state monitoring cycle")
            
            # ÊâßË°åÊï∞ÊçÆÂ∫ìÁ∫ßÂà´ÁöÑÁä∂ÊÄÅÊ£ÄÊü•
            await check_and_recover_stuck_states(stuck_monitor_logger)
            
            stuck_monitor_logger.info("Stuck state monitoring cycle completed")
            
        except Exception as e:
            stuck_monitor_logger.error(f"Error in stuck state monitor: {e}", exc_info=True)
            await asyncio.sleep(60)  # Âá∫ÈîôÊó∂Á≠âÂæÖ1ÂàÜÈíüÂÜçÈáçËØï

async def check_and_recover_stuck_states(logger):
    """
    Ê£ÄÊü•Âπ∂ÊÅ¢Â§çÂç°‰ΩèÁöÑÁä∂ÊÄÅ
    """
    try:
        from database.connection import get_db_context
        from sqlalchemy import text
        from datetime import datetime, timedelta
        
        # ‰ΩøÁî®‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®ÂàõÂª∫Êï∞ÊçÆÂ∫ì‰ºöËØù
        with get_db_context() as db:
            try:
                # Êü•ËØ¢checkpointsË°®‰∏≠ÁöÑÂ§ÑÁêÜÁä∂ÊÄÅ
                query = """
                SELECT DISTINCT thread_id, 
                       checkpoint->>'dialog_state' as dialog_state,
                       checkpoint->>'current_step_description' as step_description,
                       checkpoint->'messages' as messages,
                       parent_checkpoint_id,
                       type,
                       checkpoint
                FROM checkpoints 
                WHERE checkpoint->>'dialog_state' IN (
                    'generation_failed',
                    'sas_generating_individual_xmls',
                    'parameter_mapping',
                    'merge_xml',
                    'error'
                )
                AND checkpoint_id IN (
                    SELECT MAX(checkpoint_id) 
                    FROM checkpoints 
                    GROUP BY thread_id
                );
                """
                
                result = db.execute(text(query))
                stuck_flows = result.fetchall()
                
                logger.info(f"Found {len(stuck_flows)} flows in processing states")
                
                # Ê£ÄÊü•ÊØè‰∏™ÂèØËÉΩÂç°‰ΩèÁöÑflow
                for flow in stuck_flows:
                    thread_id = flow[0]
                    dialog_state = flow[1]
                    step_description = flow[2]
                    messages = flow[3]
                    
                    logger.info(f"Checking flow {thread_id} in state {dialog_state}")
                    
                    # ÁÆÄÂçïÁöÑÂêØÂèëÂºèÂà§Êñ≠ÔºöÂ¶ÇÊûúÂ§Ñ‰∫éÂ§ÑÁêÜÁä∂ÊÄÅ‰ΩÜÊ≤°ÊúâÊúÄËøëÁöÑÊ¥ªÂä®
                    should_recover = await should_auto_recover_flow(
                        thread_id, dialog_state, step_description, messages, logger
                    )
                    
                    if should_recover:
                        logger.warning(f"Auto-recovering stuck flow {thread_id}")
                        await auto_recover_flow(thread_id, dialog_state, logger)
                        
            except Exception as e:
                logger.error(f"Error during stuck state check: {e}", exc_info=True)
                # ‰∏çÈúÄË¶ÅÊâãÂä®ÂÖ≥Èó≠Ôºå‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂô®‰ºöËá™Âä®Â§ÑÁêÜ
            
    except Exception as e:
        logger.error(f"Error in check_and_recover_stuck_states: {e}", exc_info=True)

async def should_auto_recover_flow(thread_id, dialog_state, step_description, messages, logger):
    """
    Âà§Êñ≠ÊòØÂê¶Â∫îËØ•Ëá™Âä®ÊÅ¢Â§çflow
    """
    try:
        # ÂêØÂèëÂºèËßÑÂàôÔºö
        # 1. Â¶ÇÊûúÂ§Ñ‰∫éXMLÁîüÊàêÁä∂ÊÄÅ‰ΩÜÊ≤°Êúâstep_descriptionÔºåÂèØËÉΩÂç°‰Ωè‰∫Ü
        if dialog_state in ['generating_xml_relation', 'generating_xml_final']:
            if not step_description or step_description.strip() == "":
                logger.info(f"Flow {thread_id}: No step description in XML generation state")
                return True
        
        # 2. Â¶ÇÊûúmessages‰∏∫Á©∫ÊàñÂæàÂ∞ëÔºåÂèØËÉΩË°®Á§∫Ê≤°ÊúâÊ≠£Â∏∏Â§ÑÁêÜ
        if not messages or (isinstance(messages, list) and len(messages) == 0):
            logger.info(f"Flow {thread_id}: Empty messages in processing state")
            return True
        
        # 3. ÂÖ∂‰ªñÂêØÂèëÂºèËßÑÂàôÂèØ‰ª•Âú®ËøôÈáåÊ∑ªÂä†
        # ÊØîÂ¶ÇÊ£ÄÊü•ÊúÄÂêéÊõ¥Êñ∞Êó∂Èó¥Á≠â
        
        return False
        
    except Exception as e:
        logger.error(f"Error checking flow {thread_id}: {e}")
        return False

async def auto_recover_flow(thread_id, dialog_state, logger):
    """
    Ëá™Âä®ÊÅ¢Â§çÂç°‰ΩèÁöÑflow
    """
    try:
        from backend.sas.graph_builder import create_robot_flow_graph
        from backend.langgraphchat.llms.deepseek_client import DeepSeekLLM
        
        # Ëé∑ÂèñLLMÂÆû‰æãÂíåSAS app
        llm = DeepSeekLLM()
        sas_app = create_robot_flow_graph(llm=llm, checkpointer=app.state.checkpointer_instance)
        
        config = {"configurable": {"thread_id": thread_id}}
        
        # Ëé∑ÂèñÂΩìÂâçÁä∂ÊÄÅ
        state_snapshot = await sas_app.aget_state(config)
        if not state_snapshot or not hasattr(state_snapshot, 'values'):
            logger.warning(f"Cannot recover flow {thread_id}: no state found")
            return
        
        current_state = state_snapshot.values
        
        # Ê†πÊçÆÂΩìÂâçÁä∂ÊÄÅÂÜ≥ÂÆöÊÅ¢Â§çÁ≠ñÁï•
        if dialog_state in ['generating_xml_relation', 'generating_xml_final']:
            # XMLÁîüÊàêÂç°‰ΩèÔºåÂ∞ùËØïËÆæÁΩÆ‰∏∫ÂÆåÊàêÁä∂ÊÄÅ
            recovered_state = {
                **current_state,
                'dialog_state': 'sas_step3_completed',
                'completion_status': 'completed_success',
                'is_error': False,
                'error_message': None,
                'current_step_description': 'Auto-recovered from stuck XML generation state',
                'final_flow_xml_path': current_state.get('final_flow_xml_path') or f'/tmp/flow_{thread_id}_auto_recovered.xml'
            }
            
            await sas_app.aupdate_state(config, recovered_state)
            logger.info(f"Auto-recovered flow {thread_id} from {dialog_state} to completed state")
            
        else:
            # ÂÖ∂‰ªñÂ§ÑÁêÜÁä∂ÊÄÅÔºåÈáçÁΩÆ‰∏∫ÂàùÂßãÁä∂ÊÄÅ
            reset_state = {
                **current_state,
                'dialog_state': 'initial',
                'completion_status': None,
                'is_error': False,
                'error_message': None,
                'current_step_description': 'Auto-recovered from stuck processing state',
                'task_list_accepted': False,
                'module_steps_accepted': False,
                'revision_iteration': 0
            }
            
            await sas_app.aupdate_state(config, reset_state)
            logger.info(f"Auto-recovered flow {thread_id} from {dialog_state} to initial state")
            
    except Exception as e:
        logger.error(f"Failed to auto-recover flow {thread_id}: {e}", exc_info=True)

# Êñ∞Â¢ûÔºöÂ∫îÁî®ÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜ
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Â∫îÁî®ÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜ"""
    # ÂêØÂä®Êó∂ÁöÑÊìç‰Ωú
    startup_logger = logging.getLogger("backend.app.lifespan")
    
    # ÂêØÂä®Áé∞ÊúâÁöÑstartup‰∫ã‰ª∂ - ‰øÆÂ§çÂáΩÊï∞Ë∞ÉÁî®
    await startup_event()  # Ë∞ÉÁî®ÂéüÊúâÁöÑstartup_eventÂáΩÊï∞
    # initialize_checkpointer Âíå validate_api_configuration Â∑≤ÁªèÂú®startup_event‰∏≠Ë∞ÉÁî®‰∫Ü
    
    # ÂêØÂä®ÂêéÂè∞ÁõëÊéß‰ªªÂä°
    monitor_task = None
    try:
        monitor_task = asyncio.create_task(stuck_state_monitor_task())
        startup_logger.info("Started stuck state monitor task")
        
        yield  # Â∫îÁî®ËøêË°åÊúüÈó¥
        
    finally:
        # ÂÖ≥Èó≠Êó∂ÁöÑÊìç‰Ωú
        startup_logger.info("Shutting down application...")
        
        # ÂÅúÊ≠¢ÁõëÊéß‰ªªÂä°
        if monitor_task and not monitor_task.done():
            monitor_task.cancel()
            try:
                await monitor_task
            except asyncio.CancelledError:
                startup_logger.info("Stuck state monitor task cancelled")
        
        # ÂÖ≥Èó≠checkpointer
        await shutdown_checkpointer()
        startup_logger.info("Application shutdown complete")

# Initialize FastAPI app (Keep this section)
app = FastAPI(
    title="Flow Editor",
    description="‰∏Ä‰∏™Âü∫‰∫éReact FlowÂíåLangGraphÁöÑÂèØËßÜÂåñÂ∑•‰ΩúÊµÅÁºñËæëÂô®",
    version="1.0.0",
    lifespan=lifespan  # ‰ΩøÁî®Êñ∞ÁöÑÁîüÂëΩÂë®ÊúüÁÆ°ÁêÜ
)

# Ê∑ªÂä†Ëøô‰∏™‰∏≠Èó¥‰ª∂Êù•ËÆ∞ÂΩïÊâÄÊúâËØ∑Ê±Ç (Modify to use a specific logger)
@app.middleware("http")
async def log_requests_detailed(request: Request, call_next):
    # Get a specific logger for HTTP requests, it will inherit from 'backend.app' -> 'backend'
    http_logger = logging.getLogger("backend.app.http_requests") # Specific logger for http requests
    
    # client_host = request.client.host if request.client else "Unknown" # Covered by logger format
    http_logger.info(f"REQUEST: {request.method} {request.url.path} from {request.client.host if request.client else 'Unknown Client'}")
    # For more detail, use DEBUG level:
    # http_logger.debug(f"Headers: {dict(request.headers)}")

    start_time = time.time()
    try:
        response = await call_next(request) # Ë∞ÉÁî®ÂêéÁª≠Â§ÑÁêÜÊàñË∑ØÁî±
        process_time = time.time() - start_time
        http_logger.info(f"RESPONSE: {request.method} {request.url.path} - STATUS {response.status_code} (took: {process_time:.4f}s)")
        # http_logger.debug(f"Response Headers: {dict(response.headers)}")
    except Exception as e:
        process_time = time.time() - start_time
        # Log error with exception info
        http_logger.error(f"ERROR: {request.method} {request.url.path} - Exception {type(e).__name__} (took: {process_time:.4f}s)", exc_info=True)
        raise e from None # Re-raise to let FastAPI handle it
    return response

# CORS configuration (Keep this section)
app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"^http://192\.168\.16\.\d{1,3}:(3000|3001|8000|8080)$|^http://(localhost|127\.0\.0\.1):(3000|3001|8000|8080)$",
    allow_origins=APP_CONFIG['CORS_ORIGINS'],
    allow_credentials=APP_CONFIG.get('CORS_CREDENTIALS', True),
    allow_methods=APP_CONFIG.get('CORS_METHODS', ["*"]),
    allow_headers=APP_CONFIG.get('CORS_HEADERS', ["*"]),
    expose_headers=["*"],
)

logger.info("Registering routers...") # Keep
# Include routers (Keep this section)
try:
    app.include_router(user.router)
    logger.info("Registered user router.")
    app.include_router(flow.router)
    logger.info("Registered flow router.")
    app.include_router(email.router)
    logger.info("Registered email router.")
    app.include_router(auth.router)
    logger.info("Registered auth router.")
    app.include_router(node_templates.router)
    logger.info("Registered node_templates router.")
    
    if not MINIMAL_MODE:
        try:
            # app.include_router(workflow_router.router) # Example
            logger.info("Registered workflow router (if applicable).")
        except Exception as e:
            logger.error(f"Failed to register workflow router: {e}")
    else:
        logger.info("Skipped registering workflow router (minimal mode).")

    app.include_router(flow_variables.router)
    logger.info("Registered flow_variables router.")
    app.include_router(chat.router)
    logger.info("Registered chat router.")
    app.include_router(sas_chat.router)
    logger.info("Registered sas_chat router.")
            
except Exception as e:
    logger.error(f"Error registering routers: {e}", exc_info=True)
    raise

logger.info("FastAPI application initialization complete. Ready for requests...") # Keep

# Ê≥®ÈáäÊéâÂéüÊúâÁöÑ‰∫ã‰ª∂Ë£ÖÈ•∞Âô®ÔºåÂõ†‰∏∫Áé∞Âú®‰ΩøÁî®lifespanÁÆ°ÁêÜ
# @app.on_event("startup") # Keep this section
async def startup_event():
    log_id = str(uuid.uuid4())
    current_time = datetime.now().isoformat()
    # Use the main app logger or a specific startup logger
    startup_logger = logging.getLogger("backend.app.startup_event")
    startup_logger.info(f"üöÄ STARTUP EVENT 1 (startup_event) CALLED - ID: {log_id} at {current_time}")
    template_service = get_node_template_service()
    startup_logger.info(f"Node templates loaded by startup_event (ID: {log_id}).")
    
    # Ë∞ÉÁî®ÂÖ∂‰ªñÂêØÂä®ÂáΩÊï∞
    await initialize_checkpointer()
    await validate_api_configuration()

# @app.on_event("startup")
async def initialize_checkpointer():
    """
    Initializes the LangGraph AsyncPostgresSaver checkpointer instance at application startup
    and stores both the context manager and the instance in app.state.
    """
    checkpointer_logger = logging.getLogger("backend.app.checkpointer_init")
    app.state.saver_context_manager = None  # Initialize to None
    app.state.checkpointer_instance = None  # Initialize to None
    try:
        db_url = DB_CONFIG.get('DATABASE_URL')
        # Log the actual DB_URL being used
        checkpointer_logger.info(f"Attempting to initialize checkpointer with DATABASE_URL: {db_url}")
        if db_url:
            # Convert SQLAlchemy format to standard PostgreSQL format for AsyncPostgresSaver
            # Remove the "+psycopg2" dialect from the URL
            if db_url.startswith('postgresql+psycopg2://'):
                db_url_for_checkpointer = db_url.replace('postgresql+psycopg2://', 'postgresql://')
                checkpointer_logger.info(f"Converted URL for AsyncPostgresSaver: {db_url_for_checkpointer}")
            else:
                db_url_for_checkpointer = db_url
            
            # Get the async context manager
            app.state.saver_context_manager = AsyncPostgresSaver.from_conn_string(db_url_for_checkpointer)
            # Enter the context manager to get the actual instance
            app.state.checkpointer_instance = await app.state.saver_context_manager.__aenter__()
            # Now call setup on the actual instance
            await app.state.checkpointer_instance.setup()
            checkpointer_logger.info("Successfully initialized AsyncPostgresSaver (checkpointer) and stored in app.state.")
        else:
            checkpointer_logger.error("DATABASE_URL not found in DB_CONFIG. Checkpointer cannot be initialized.")
    except Exception as e:
        checkpointer_logger.error(f"Error initializing AsyncPostgresSaver (checkpointer): {e}", exc_info=True)
        # If initialization failed, ensure instance is None so get_checkpointer dependency fails cleanly
        app.state.checkpointer_instance = None 
        if app.state.saver_context_manager: # If context manager was created but enter/setup failed
            try:
                # Attempt to clean up the context manager if __aenter__ was called or partially succeeded
                # This might not be strictly necessary if __aenter__ itself failed, but good for robustness
                await app.state.saver_context_manager.__aexit__(type(e), e, e.__traceback__)
            except Exception as exit_e:
                checkpointer_logger.error(f"Error during __aexit__ in checkpointer initialization failure: {exit_e}", exc_info=True)
            app.state.saver_context_manager = None # Ensure it's None after failed attempt

# @app.on_event("shutdown")
async def shutdown_checkpointer():
    """
    Cleans up the LangGraph checkpointer resources on application shutdown.
    """
    shutdown_logger = logging.getLogger("backend.app.checkpointer_shutdown")
    if hasattr(app.state, 'saver_context_manager') and app.state.saver_context_manager is not None:
        shutdown_logger.info("Shutting down AsyncPostgresSaver (checkpointer)...")
        try:
            # Call __aexit__ on the stored context manager
            # Pass None, None, None for a clean exit
            await app.state.saver_context_manager.__aexit__(None, None, None)
            shutdown_logger.info("AsyncPostgresSaver (checkpointer) shutdown successfully.")
        except Exception as e:
            shutdown_logger.error(f"Error during AsyncPostgresSaver (checkpointer) shutdown: {e}", exc_info=True)
        finally:
            app.state.saver_context_manager = None
            app.state.checkpointer_instance = None
    else:
        shutdown_logger.info("AsyncPostgresSaver (checkpointer) context manager not found in app.state, skipping shutdown.")

@app.get("/") # Keep this section
async def root():
    return {"message": "Flow Editor API"}

@app.get("/version") # ÁßªÈô§ /api ÂâçÁºÄ
async def version(request: Request):
    version_logger = logging.getLogger("backend.app.version_endpoint")
    origin = request.headers.get("origin", "Unknown Origin")
    # version_logger.info(f"Version request received from: {origin}") # Example of logging
    version_data = get_version_info()
    # version_logger.info(f"Returning version info: {version_data}") # Example of logging
    return version_data

# @app.on_event("startup") # Keep this section (ensure it's distinct if multiple startup events)
async def validate_api_configuration():
    log_id = str(uuid.uuid4())
    current_time = datetime.now().isoformat()
    config_validation_logger = logging.getLogger("backend.app.config_validation")
    config_validation_logger.info(f"üöÄ STARTUP EVENT 2 (validate_api_configuration) CALLED - ID: {log_id} at {current_time}")
    
    from backend.config import APP_CONFIG as app_cfg, AI_CONFIG, DB_CONFIG # Renamed to avoid conflict
    
    if AI_CONFIG.get('USE_DEEPSEEK', False): # Use .get for safety
        config_validation_logger.info("Validating DeepSeek API configuration...")
        api_key = AI_CONFIG.get('DEEPSEEK_API_KEY')
        invalid_key = not api_key or api_key == "your_deepseek_api_key_here" or api_key.startswith("sk-if-you-see-this")
        
        if invalid_key:
            config_validation_logger.warning("‚ö†Ô∏è No valid DeepSeek API key set. Please set DEEPSEEK_API_KEY environment variable.")
        else:
            config_validation_logger.info(f"‚úì DeepSeek API key is set (ends with: ...{api_key[-4:] if api_key else 'N/A'}).") # Masked
            
        base_url = AI_CONFIG.get('DEEPSEEK_BASE_URL', '').rstrip('/')
        config_validation_logger.info(f"DeepSeek API Base URL: {base_url}")
        
        if '/v1/' in base_url or base_url.endswith('/v1'):
            config_validation_logger.warning(f"‚ö†Ô∏è Base URL contains /v1: {base_url}. This might cause duplicate paths as code often adds /v1/chat/completions.")
            
        config_validation_logger.info(f"DeepSeek Model: {AI_CONFIG.get('DEEPSEEK_MODEL')}")
        
        try:
            from backend.langgraphchat.llms.deepseek_client import DeepSeekLLM # Verify import
            config_validation_logger.info("‚úì DeepSeekLLM client module imported successfully.")
        except Exception as e:
            config_validation_logger.error(f"‚ö†Ô∏è Failed to import or validate DeepSeekLLM client module: {str(e)}", exc_info=True)
    
    db_url = DB_CONFIG.get('DATABASE_URL')
    config_validation_logger.info(f"Database URL: {'Set' if db_url else 'Not Set (Using default or in-memory if applicable)'}") # Simplified
    
    if app_cfg.get('DEBUG', False): # Use .get for safety
        config_validation_logger.info("‚ö†Ô∏è Debug mode is ENABLED.")
    else:
        config_validation_logger.info("‚úì Debug mode is DISABLED.")
        
    config_validation_logger.info(f"API Prefix: {app_cfg.get('API_PREFIX')}")
    config_validation_logger.info(f"CORS Allowed Origins: {', '.join(app_cfg.get('CORS_ORIGINS', []))}")

# Ensure all logger calls like print() are replaced with logger.info(), logger.debug() etc.
# For example, in /api/version, print() statements should be logger calls.
