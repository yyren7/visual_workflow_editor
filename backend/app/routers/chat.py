from typing import List, Dict, Any, Optional, AsyncGenerator
from fastapi import APIRouter, Depends, HTTPException, Query, status, Request, Response, BackgroundTasks
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from sqlalchemy.orm import Session
import logging
import json
import asyncio # ç¡®ä¿å¯¼å…¥ asyncio
import time  # æ·»åŠ æ—¶é—´æ¨¡å—å¯¼å…¥
from datetime import datetime # <--- ä¿®æ”¹æ­¤è¡Œ
from collections import defaultdict # å¯¼å…¥ defaultdict
from backend.langgraphchat.context import current_flow_id_var # <--- Import context variable
import uuid # ç¡®ä¿å¯¼å…¥

from backend.app import schemas
from database.connection import get_db, get_db_context
from backend.app.utils import get_current_user, verify_flow_ownership
from backend.app.services.chat_service import ChatService
from backend.app.services.flow_service import FlowService
from database.models import Flow
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, AIMessageChunk

logger = logging.getLogger(__name__)

# --- æ–°å¢ï¼šç”¨äºå­˜å‚¨æ´»åŠ¨äº‹ä»¶æµçš„å†…å­˜é˜Ÿåˆ— --- 
# Key: chat_id, Value: asyncio.Queue
# æ³¨æ„ï¼šç®€å•å†…å­˜å®ç°ï¼Œä¸é€‚ç”¨äºå¤šè¿›ç¨‹/å¤šå®ä¾‹éƒ¨ç½²
active_chat_queues: Dict[str, asyncio.Queue] = defaultdict(asyncio.Queue)
# ç”¨äºè¿½è¸ªæ¯ä¸ªchatçš„SSEè¿æ¥æ•°
active_sse_connections: Dict[str, int] = defaultdict(int)
# ç”¨äºé€šçŸ¥ GET è¯·æ±‚æµå·²ç»“æŸçš„æ ‡è®°
STREAM_END_SENTINEL = {"type": "stream_end", "data": {"message": "Stream finished or no stream generated."}}
# é˜Ÿåˆ—æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢å†…å­˜æ— é™å¢é•¿
MAX_QUEUE_SIZE = 100 
# --- ç»“æŸæ–°å¢ ---

router = APIRouter(
    prefix="/chats",
    tags=["chats"],
    responses={404: {"description": "Not found"}},
)

# --- è¾…åŠ©å‡½æ•°ï¼šå°†æ•°æ®åº“æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º Langchain æ ¼å¼ ---
def _format_messages_to_langchain(messages: List[Dict]) -> List[BaseMessage]:
    """å°†åŒ…å« 'role' å’Œ 'content' çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º Langchain BaseMessage åˆ—è¡¨ã€‚"""
    langchain_messages = []
    for message in messages:
        role = message.get("role")
        content = message.get("content", "")
        if role == "user":
            langchain_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            langchain_messages.append(AIMessage(content=content))
        # å¯ä»¥é€‰æ‹©æ€§åœ°å¤„ç†å…¶ä»– roleï¼Œä¾‹å¦‚ 'system'
        # else:
        #     logger.warning(f"Unknown message role '{role}' encountered during formatting.")
    return langchain_messages

@router.post("/", response_model=schemas.Chat)
async def create_chat(
    chat: schemas.ChatCreate, 
    request: Request,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    åˆ›å»ºæ–°çš„èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½ä¸ºè‡ªå·±çš„æµç¨‹å›¾åˆ›å»ºèŠå¤©
    """
    logger.info(f"æˆåŠŸè¿›å…¥ create_chat å‡½æ•°: {request.method} {request.url.path}")

    try:
        # éªŒè¯æµç¨‹å›¾å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        logger.info(f"éªŒè¯ flow ownership: {chat.flow_id}")
        verified_flow = verify_flow_ownership(chat.flow_id, current_user, db)
        logger.info(f"Flow ownership éªŒè¯é€šè¿‡ for flow: {verified_flow.id}")

        # åˆ›å»ºèŠå¤©
        logger.info(f"è°ƒç”¨ chat_service.create_chat")
        chat_service = ChatService(db)
        db_chat = chat_service.create_chat(
            flow_id=chat.flow_id,
            name=chat.name,
            chat_data=chat.chat_data
        )
        logger.info(f"chat_service.create_chat è¿”å›: {db_chat}")

        if not db_chat:
            logger.error("chat_service.create_chat æœªæˆåŠŸåˆ›å»ºèŠå¤©ï¼Œå¼•å‘ 400")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ— æ³•åˆ›å»ºèŠå¤©"
            )

        # --- Update Flow's last_interacted_chat_id ---
        if verified_flow: # Ensure we have the flow object
            try:
                verified_flow.last_interacted_chat_id = db_chat.id
                db.commit()
                logger.info(f"æ›´æ–° Flow {verified_flow.id} çš„ last_interacted_chat_id ä¸º {db_chat.id}")
            except Exception as update_err:
                logger.error(f"æ›´æ–° Flow last_interacted_chat_id å¤±è´¥ (Flow: {verified_flow.id}, Chat: {db_chat.id}): {update_err}", exc_info=True)
                db.rollback() # Rollback only the failed update
        else:
             logger.warning(f"æ— æ³•æ›´æ–° Flow çš„ last_interacted_chat_idï¼Œå› ä¸ºåœ¨åˆ›å»ºèŠå¤©åæœªèƒ½è·å– Flow å¯¹è±¡ (Flow ID: {chat.flow_id})")
        # --- End Update ---

        logger.info(f"æˆåŠŸåˆ›å»ºèŠå¤©ï¼Œè¿”å› chat å¯¹è±¡ ID: {db_chat.id}")
        return db_chat
    except HTTPException as http_exc:
        logger.error(f"å¤„ç† create_chat æ—¶å‘ç”Ÿ HTTPException: {http_exc.status_code} - {http_exc.detail}")
        raise http_exc
    except Exception as e:
        logger.error(f"å¤„ç† create_chat æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ›å»ºèŠå¤©æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.get("/{chat_id}", response_model=schemas.Chat)
async def get_chat(
    chat_id: str, 
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    è·å–èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½è®¿é—®è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    return chat


@router.get("/flow/{flow_id}", response_model=List[schemas.Chat])
async def get_flow_chats(
    flow_id: str, 
    skip: int = Query(0, ge=0), 
    limit: int = Query(100, le=1000),
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    è·å–æµç¨‹å›¾çš„æ‰€æœ‰èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½è®¿é—®è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(flow_id, current_user, db)
    
    # è·å–èŠå¤©åˆ—è¡¨
    chat_service = ChatService(db)
    chats = chat_service.get_chats_for_flow(flow_id, skip, limit)
    
    return chats


@router.put("/{chat_id}", response_model=schemas.Chat)
async def update_chat(
    chat_id: str,
    chat_update: schemas.ChatUpdate,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    æ›´æ–°èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½æ›´æ–°è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    # æ›´æ–°èŠå¤©
    updated_chat = chat_service.update_chat(
        chat_id=chat_id,
        name=chat_update.name,
        chat_data=chat_update.chat_data
    )
    
    if not updated_chat:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æ›´æ–°èŠå¤©å¤±è´¥"
        )
    
    return updated_chat


@router.put("/{chat_id}/messages/{message_timestamp}", status_code=status.HTTP_202_ACCEPTED)
async def edit_user_message(
    chat_id: str,
    message_timestamp: str, 
    edit_data: schemas.ChatMessageEdit, 
    db: Session = Depends(get_db),
    current_user: schemas.User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """
    ç¼–è¾‘ç”¨æˆ·æ¶ˆæ¯, åˆ é™¤æ­¤æ¶ˆæ¯ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯, å¹¶ä»¥æ–°å†…å®¹é‡æ–°ç”Ÿæˆç”¨æˆ·æ¶ˆæ¯ã€‚
    ç„¶ååƒæ–°æ¶ˆæ¯ä¸€æ ·è§¦å‘ LangGraph å·¥ä½œæµã€‚
    å¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½æ“ä½œè‡ªå·±æµç¨‹å›¾çš„èŠå¤©ã€‚
    ç«‹å³è¿”å› 202 Acceptedï¼Œå®¢æˆ·ç«¯éœ€è¦éšåè¿æ¥ GET /{chat_id}/events è·å–äº‹ä»¶ã€‚
    """
    logger.info(f"Attempting to edit message {message_timestamp} in chat {chat_id} and trigger workflow.")
    chat_service = ChatService(db)
    chat_before_edit = chat_service.get_chat(chat_id)

    if not chat_before_edit:
        logger.error(f"Chat {chat_id} not found for editing message.")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    verify_flow_ownership(chat_before_edit.flow_id, current_user, db)
    logger.debug(f"Ownership verified for flow {chat_before_edit.flow_id}")

    # åœ¨è°ƒç”¨æœåŠ¡å±‚ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å­˜åœ¨
    message_found = False
    if chat_before_edit.chat_data and "messages" in chat_before_edit.chat_data:
        messages = chat_before_edit.chat_data.get("messages", [])
        if isinstance(messages, list): # ç¡®ä¿ messages æ˜¯åˆ—è¡¨
            for msg in messages:
                if isinstance(msg, dict) and msg.get("timestamp") == message_timestamp and msg.get("role") == "user":
                    message_found = True
                    break
    
    if not message_found:
        logger.error(f"User message with timestamp {message_timestamp} not found in chat {chat_id} before calling service.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"è¦ç¼–è¾‘çš„æ¶ˆæ¯ä¸å­˜åœ¨æˆ–æ—¶é—´æˆ³ä¸åŒ¹é… (ts: {message_timestamp})"
        )

    updated_chat = chat_service.edit_user_message_and_truncate(
        chat_id=chat_id,
        message_timestamp=message_timestamp,
        new_content=edit_data.new_content
    )

    if not updated_chat:
        logger.error(f"Failed to edit message {message_timestamp} in chat {chat_id} via service.")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, # æ”¹ä¸º500ï¼Œå› ä¸ºæ­¤æ—¶æ›´å¯èƒ½æ˜¯å†…éƒ¨é—®é¢˜
            detail="ç¼–è¾‘æ¶ˆæ¯æ—¶å‘ç”Ÿå†…éƒ¨æœåŠ¡å™¨é”™è¯¯"
        )
    
    logger.info(f"Successfully edited message {message_timestamp} in chat {chat_id}. DB state updated.")

    # --- è§¦å‘åå°äº‹ä»¶å¤„ç† ---
    event_queue = asyncio.Queue(maxsize=MAX_QUEUE_SIZE) 
    active_chat_queues[chat_id] = event_queue
    logger.info(f"ä¸º chat {chat_id} (after edit) åˆ›å»º/è®¾ç½®äº†æ–°çš„äº‹ä»¶é˜Ÿåˆ—")

    # The initial_user_message_content is not strictly needed by _process_and_publish_chat_events
    # when is_edit_flow is True, as it will read the latest from DB.
    background_tasks.add_task(
        _process_and_publish_chat_events, 
        chat_id, 
        initial_user_message_content=None, # Content is already in DB
        event_queue=event_queue,
        is_edit_flow=True
    )
    logger.info(f"å·²ä¸º chat {chat_id} (after edit) å¯åŠ¨åå°äº‹ä»¶å¤„ç†ä»»åŠ¡")

    return Response(status_code=status.HTTP_202_ACCEPTED)


@router.post("/{chat_id}/messages")
async def add_message(
    chat_id: str,
    message: schemas.ChatAddMessage,
    db: Session = Depends(get_db),
    current_user: schemas.User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
) -> Response:
    """
    å‘èŠå¤©æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œè§¦å‘åå°å¤„ç†æµç¨‹ã€‚
    æ”¯æŒè™šæ‹ŸChat IDï¼ˆä½¿ç”¨flow_idä½œä¸ºchat_idï¼‰ã€‚
    ç«‹å³è¿”å› 202 Acceptedï¼Œå®¢æˆ·ç«¯éœ€è¦éšåè¿æ¥ GET /{chat_id}/events è·å–äº‹ä»¶ã€‚
    """
    chat_service = ChatService(db)
    flow_service = FlowService(db)
    
    # é¦–å…ˆå°è¯•è·å–å¸¸è§„èŠå¤©
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        # æ£€æŸ¥chat_idæ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„LangGraphè™šæ‹ŸChat ID
        logger.info(f"Chat {chat_id} not found, checking if it's a virtual LangGraph chat ID...")
        
        # ä»chat_idä¸­è§£æflow_idã€task_indexã€detail_index
        # æ”¯æŒæ ¼å¼ï¼šflow_id, flow_id_task_X, flow_id_task_X_detail_Y
        flow_id = chat_id.split('_task_')[0].split('_detail_')[0]
        task_index = None
        detail_index = None
        
        # è§£ætask_index
        if '_task_' in chat_id:
            task_part = chat_id.split('_task_')[1]
            if '_detail_' in task_part:
                task_index = int(task_part.split('_detail_')[0])
                detail_index = int(task_part.split('_detail_')[1])
            else:
                task_index = int(task_part)
        
        # å°è¯•éªŒè¯è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„flow_id
        try:
            flow = verify_flow_ownership(flow_id, current_user, db)
            if flow:
                logger.info(f"Virtual LangGraph chat detected: {chat_id} -> flow_id: {flow_id}, task: {task_index}, detail: {detail_index}")
                
                # ä¸ºè™šæ‹ŸèŠå¤©åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„èŠå¤©ä¼šè¯
                # è¿™æ ·å¯ä»¥å¤ç”¨ç°æœ‰çš„èŠå¤©å¤„ç†é€»è¾‘
                if task_index is not None and detail_index is not None:
                    virtual_chat_name = f"Virtual Detail Chat - {flow.name} Task {task_index + 1} Detail {detail_index + 1}"
                elif task_index is not None:
                    virtual_chat_name = f"Virtual Task Chat - {flow.name} Task {task_index + 1}"
                else:
                    virtual_chat_name = f"Virtual LangGraph Chat - {flow.name}"
                
                virtual_chat = chat_service.create_chat(
                    flow_id=flow_id,  # ä½¿ç”¨è§£æå‡ºçš„flow_id
                    name=virtual_chat_name,
                    chat_data={
                        "messages": [], 
                        "is_virtual_langgraph_chat": True,
                        "virtual_chat_id": chat_id,  # ä¿å­˜åŸå§‹çš„è™šæ‹Ÿchat_id
                        "task_index": task_index,
                        "detail_index": detail_index
                    }
                )
                
                if virtual_chat:
                    logger.info(f"Created virtual chat {virtual_chat.id} for LangGraph chat_id {chat_id}")
                    # ä½¿ç”¨æ–°åˆ›å»ºçš„è™šæ‹ŸèŠå¤©
                    chat = virtual_chat
                    # é‡è¦ï¼šä½¿ç”¨å®é™…åˆ›å»ºçš„chat IDï¼Œè€Œä¸æ˜¯è™šæ‹Ÿchat ID
                    actual_chat_id = virtual_chat.id
                else:
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail="æ— æ³•åˆ›å»ºè™šæ‹ŸèŠå¤©ä¼šè¯"
                    )
            else:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨ä¸”ä¸æ˜¯æœ‰æ•ˆçš„æµç¨‹å›¾ID")
        except HTTPException as he:
            # å¦‚æœéªŒè¯å¤±è´¥ï¼Œé‡æ–°æŠ›å‡ºåŸå§‹çš„404é”™è¯¯
            if he.status_code == 404:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨")
            else:
                raise he
        except Exception as e:
            logger.error(f"Error checking virtual LangGraph chat ID {chat_id}: {e}")
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨")
    else:
        # å¸¸è§„èŠå¤©æµç¨‹
        actual_chat_id = chat_id
    
    # éªŒè¯æµç¨‹å›¾å½’å±ï¼ˆå¸¸è§„èŠå¤©å’Œè™šæ‹ŸèŠå¤©éƒ½éœ€è¦ï¼‰
    verified_flow = verify_flow_ownership(chat.flow_id, current_user, db)
    logger.info(f"Flow ownership éªŒè¯é€šè¿‡ for flow: {verified_flow.id} linked to chat: {chat.id}")
    
    if message.role != 'user':
         logger.warning(f"Received message with role '{message.role}' in add_message. Processing as user message.")
         
    # å¯¹äºè™šæ‹ŸèŠå¤©ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åŸå§‹çš„chat_idï¼ˆå³flow_idï¼‰ä½œä¸ºäº‹ä»¶é˜Ÿåˆ—çš„key
    # è¿™æ ·å‰ç«¯å°±èƒ½ç”¨flow_idæ¥ç›‘å¬äº‹ä»¶
    event_queue_key = chat_id  # ä½¿ç”¨åŸå§‹è¯·æ±‚çš„chat_id
    actual_processing_chat_id = chat.id  # ä½¿ç”¨å®é™…çš„chatè®°å½•IDè¿›è¡Œå¤„ç†
    
    event_queue = asyncio.Queue(maxsize=MAX_QUEUE_SIZE) 
    active_chat_queues[event_queue_key] = event_queue  # ä½¿ç”¨åŸå§‹chat_idä½œä¸ºkey
    logger.info(f"ä¸º chat {event_queue_key} (actual: {actual_processing_chat_id}) åˆ›å»º/è®¾ç½®äº†æ–°çš„äº‹ä»¶é˜Ÿåˆ—")

    background_tasks.add_task(
        _process_and_publish_chat_events, 
        actual_processing_chat_id,  # ä¼ é€’å®é™…çš„chat IDç»™åå°ä»»åŠ¡
        initial_user_message_content=message.content, 
        event_queue=event_queue,
        is_edit_flow=False,
        client_message_id=message.client_message_id
    )
    logger.info(f"å·²ä¸º chat {event_queue_key} (processing: {actual_processing_chat_id}) å¯åŠ¨åå°äº‹ä»¶å¤„ç†ä»»åŠ¡ (new message, client_id: {message.client_message_id})")
    
    return Response(status_code=status.HTTP_202_ACCEPTED)


@router.get("/{chat_id}/events")
async def get_chat_events(chat_id: str, request: Request):
    route_hit_id = str(uuid.uuid4())
    logger.info(f"â¡ï¸ Route /events hit for chat_id: {chat_id}, HIT_ID: {route_hit_id}, Client: {request.client.host if request.client else 'unknown'}")

    if chat_id not in active_chat_queues:
        logger.warning(f"â¡ï¸ Route /events: No active event queue for chat {chat_id} (HIT_ID: {route_hit_id}). Sending immediate end.")
        async def immediate_end_stream():
            yield {"event": "stream_end", "data": json.dumps({"message": f"No active event stream for chat {chat_id}."})}
        return EventSourceResponse(immediate_end_stream())

    event_queue = active_chat_queues[chat_id]
    logger.info(f"â¡ï¸ Route /events: Found event queue for chat {chat_id} (HIT_ID: {route_hit_id})")

    async def sse_event_generator(current_chat_id: str, req: Request, hit_id_from_route: str):
        sse_instance_id = str(uuid.uuid4())
        logger.error(f"ğŸš€ğŸš€ğŸš€ SSE_GENERATOR_INSTANCE_STARTED for chat_id: {current_chat_id}, HIT_ID: {hit_id_from_route}, INSTANCE_ID: {sse_instance_id}, URL: {req.url}")

        active_sse_connections[current_chat_id] += 1
        connection_count = active_sse_connections[current_chat_id]
        logger.info(f"ğŸ”´ Running SSE event sender #{connection_count} for chat {current_chat_id} (HIT_ID: {hit_id_from_route}, Instance: {sse_instance_id})")
        
        client_disconnected = False
        
        async def check_disconnect_internal():
            nonlocal client_disconnected
            if client_disconnected:
                return True
            try:
                is_disconnected = await req.is_disconnected()
                if is_disconnected:
                    client_disconnected = True
                    logger.info(f"ğŸ”´ SSE client for chat {current_chat_id} DISCONNECTED (Instance: {sse_instance_id}, HIT_ID: {hit_id_from_route}).")
                    return True
                return False
            except Exception as e_disconnect:
                logger.warning(f"ğŸ”´ Could not check client disconnect status for chat {current_chat_id} (Instance: {sse_instance_id}, HIT_ID: {hit_id_from_route}): {e_disconnect}")
                return False

        last_ping_time = 0

        try:
            logger.debug(f"SSE sender for {current_chat_id}: Sending initial ping (Instance: {sse_instance_id}).")
            yield {"event": "ping", "data": json.dumps({"timestamp": datetime.utcnow().isoformat(), "message": "SSE connection established"})}

            while not client_disconnected:
                event_data_item = None
                try:
                    logger.debug(f"[SSE {current_chat_id} Instance {sse_instance_id}] Waiting for event from queue...")
                    event_data_item = await asyncio.wait_for(event_queue.get(), timeout=1.0)
                    logger.debug(f"[SSE {current_chat_id} Instance {sse_instance_id}] Got event: {str(event_data_item)[:100]}")
                except asyncio.TimeoutError:
                    if await check_disconnect_internal(): break
                    current_time_for_ping = time.time()
                    if (current_time_for_ping - last_ping_time) >= 5.0:
                        logger.debug(f"[SSE {current_chat_id} Instance {sse_instance_id}] Client still connected. Sending ping.")
                        try:
                            yield {"event": "ping", "data": json.dumps({"timestamp": datetime.utcnow().isoformat(), "message": "keep-alive"})}
                            last_ping_time = current_time_for_ping
                        except Exception as ping_err:
                            logger.error(f"[SSE {current_chat_id} Instance {sse_instance_id}] Error sending ping: {ping_err}", exc_info=True)
                            client_disconnected = True
                            break
                    continue
                except asyncio.CancelledError:
                    logger.info(f"SSE event sender for chat {current_chat_id} was CANCELLED (Instance: {sse_instance_id}).")
                    client_disconnected = True
                    break
                except Exception as e_get_q:
                    logger.error(f"[SSE {current_chat_id} Instance {sse_instance_id}] Error getting event from queue: {e_get_q}", exc_info=True)
                    if await check_disconnect_internal(): break
                    try:
                        yield {"event": "error", "data": json.dumps({"message": f"Error fetching event: {str(e_get_q)}", "stage": "sse_queue_read"})}
                    except Exception as send_q_err_e:
                        logger.error(f"[SSE {current_chat_id} Instance {sse_instance_id}] Failed to send queue read error: {send_q_err_e}")
                    client_disconnected = True
                    break
                
                if event_data_item is STREAM_END_SENTINEL:
                    logger.info(f"[SSE {current_chat_id} Instance {sse_instance_id}] Received STREAM_END_SENTINEL. Sending final event.")
                    try:
                        yield {"event": STREAM_END_SENTINEL["type"], "data": json.dumps(STREAM_END_SENTINEL["data"])}
                        if not event_queue.empty():
                             event_queue.task_done()
                    except Exception as send_final_err_e:
                        logger.error(f"[SSE {current_chat_id} Instance {sse_instance_id}] Error sending stream_end sentinel: {send_final_err_e}", exc_info=True)
                    client_disconnected = True
                    break

                if isinstance(event_data_item, dict) and "type" in event_data_item and "data" in event_data_item:
                    event_type_to_send = event_data_item["type"]
                    data_payload_to_send = event_data_item["data"]
                    formatted_data_to_send = json.dumps(data_payload_to_send) if not isinstance(data_payload_to_send, str) else data_payload_to_send
                    logger.debug(f"SSE Sender for {current_chat_id} (Instance {sse_instance_id}): Sending event type '{event_type_to_send}'")
                    yield {"event": event_type_to_send, "data": formatted_data_to_send}
                else:
                    logger.warning(f"[SSE {current_chat_id} Instance {sse_instance_id}] Malformed event from queue: {event_data_item}")
                    yield {"event": "error", "data": json.dumps({"message": "Malformed event from queue.", "stage": "sse_formatting"})}
                
                if not event_queue.empty():
                    event_queue.task_done()
                await asyncio.sleep(0.01)

        except asyncio.CancelledError:
            logger.info(f"SSE event sender for chat {current_chat_id} (Instance {sse_instance_id}) was EXPLICITLY CANCELLED (outer).")
            client_disconnected = True
        except Exception as e_outer_gen:
            logger.error(f"SSE event sender for chat {current_chat_id} (Instance {sse_instance_id}) UNHANDLED outer error: {e_outer_gen}", exc_info=True)
            if not client_disconnected:
                try:
                    yield {"event": "error", "data": json.dumps({"message": f"Critical SSE sender error: {str(e_outer_gen)}", "stage": "sse_critical_outer"})}
                except Exception as send_outer_final_err:
                     logger.error(f"[SSE {current_chat_id} Instance {sse_instance_id}] Failed to send final critical outer error: {send_outer_final_err}")
        finally:
            logger.info(f"ğŸ”´ SSE event sender for chat {current_chat_id} (Instance {sse_instance_id}, HIT_ID: {hit_id_from_route}) entering FINALLY block. Client disconnected: {client_disconnected}")
            if active_sse_connections[current_chat_id] > 0:
                 active_sse_connections[current_chat_id] -= 1
            else:
                 logger.warning(f"ğŸ”´ SSE Cleanup for {current_chat_id} (Instance {sse_instance_id}): Connection count was already 0 or less before decrementing!")

            remaining_connections = active_sse_connections.get(current_chat_id, 0)
            logger.info(f"ğŸ”´ SSE event sender for chat {current_chat_id} (Instance {sse_instance_id}, HIT_ID: {hit_id_from_route}) CLEANING UP. Remaining connections: {remaining_connections}")

            if remaining_connections == 0:
                logger.info(f"ğŸ”´ No more SSE connections for chat {current_chat_id} (Instance {sse_instance_id}, HIT_ID: {hit_id_from_route}). Cleaning up queue and connection count entry.")
                active_sse_connections.pop(current_chat_id, None)
                
                if current_chat_id in active_chat_queues and active_chat_queues[current_chat_id] is event_queue:
                    if not event_queue.empty():
                        logger.warning(f"ğŸ”´ Cleaning up queue for chat {current_chat_id} (Instance {sse_instance_id}) but it's not empty. {event_queue.qsize()} items remaining (will be lost).")
                        while not event_queue.empty():
                            try:
                                event_queue.get_nowait()
                                event_queue.task_done()
                            except asyncio.QueueEmpty:
                                break
                    active_chat_queues.pop(current_chat_id, None)
                    logger.info(f"ğŸ”´ Queue for chat {current_chat_id} (Instance {sse_instance_id}) removed from active_chat_queues.")
                else:
                    logger.warning(f"ğŸ”´ Queue for chat {current_chat_id} (Instance {sse_instance_id}) was not found in active_chat_queues or was not the expected instance during final cleanup. Might have been cleaned by another process or a re-entrant call.")
            else:
                logger.warning(f"ğŸ”´ Still have {remaining_connections} SSE connections for chat {current_chat_id} (Instance {sse_instance_id}, HIT_ID: {hit_id_from_route}), NOT cleaning up queue or connection count entry fully.")
            logger.info(f"ğŸ”´ SSE event sender for chat {current_chat_id} (Instance {sse_instance_id}, HIT_ID: {hit_id_from_route}) FINISHED.")

    return EventSourceResponse(sse_event_generator(chat_id, request, route_hit_id))


@router.delete("/{chat_id}", response_model=bool)
async def delete_chat(
    chat_id: str,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    åˆ é™¤èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½åˆ é™¤è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    # åˆ é™¤èŠå¤©
    success = chat_service.delete_chat(chat_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="åˆ é™¤èŠå¤©å¤±è´¥"
        )
    
    return True 

# --- æå–å‡ºæ¥çš„åå°äº‹ä»¶å¤„ç†å‡½æ•° ---
async def _process_and_publish_chat_events(
    chat_id: str, 
    initial_user_message_content: Optional[str], 
    event_queue: asyncio.Queue,
    is_edit_flow: bool = False,
    client_message_id: Optional[str] = None
):
    """
    åå°ä»»åŠ¡ï¼šå¤„ç†èŠå¤©é€»è¾‘ï¼ˆæ·»åŠ æ¶ˆæ¯ï¼Œè°ƒç”¨LangGraphï¼‰ï¼Œå¹¶é€šè¿‡é˜Ÿåˆ—å‘å¸ƒSSEäº‹ä»¶ã€‚
    """
    logger.info(f"[Chat {chat_id}] Background task started (is_edit_flow: {is_edit_flow}). Initial content (if any): {initial_user_message_content}")
    is_error = False
    error_data = {}
    final_reply_accumulator = ""
    final_state = None
    token_cv = None 

    try:
        with get_db_context() as db_session_bg:
            logger.info(f"[Chat {chat_id}] Acquired DB session for background task.")
            chat_service_bg = ChatService(db_session_bg)
            flow_service_bg = FlowService(db_session_bg)

            chat = chat_service_bg.get_chat(chat_id)
            if not chat:
                logger.error(f"[Chat {chat_id}] Background task could not find chat.")
                await event_queue.put({"type": "error", "data": {"message": "Chat not found.", "stage": "setup"}})
                return
            
            if not is_edit_flow and initial_user_message_content is not None:
                logger.info(f"[Chat {chat_id}] Attempting to save user message to DB before agent call: {initial_user_message_content[:100]}...")
                saved_chat_obj, server_message_timestamp = chat_service_bg.add_message_to_chat(
                    chat_id=chat_id, 
                    role="user", 
                    content=initial_user_message_content
                )
                if saved_chat_obj and server_message_timestamp:
                    logger.info(f"[Chat {chat_id}] User message saved to DB with server_timestamp: {server_message_timestamp}.")
                    if client_message_id:
                        await event_queue.put({
                            "type": "user_message_saved", 
                            "data": {
                                "client_message_id": client_message_id,
                                "server_message_timestamp": server_message_timestamp,
                                "content": initial_user_message_content
                            }
                        })
                        logger.info(f"[Chat {chat_id}] Sent user_message_saved event for client_id: {client_message_id} -> server_ts: {server_message_timestamp}")
                    
                    chat = saved_chat_obj 
                else:
                    logger.error(f"[Chat {chat_id}] Failed to save user message to DB.")
                    await event_queue.put({
                        "type": "error", 
                        "data": {"message": "Failed to save user message.", "stage": "setup"}
                    })
                    return


            flow_id = chat.flow_id
            token_cv = current_flow_id_var.set(flow_id) 
            logger.info(f"[Chat {chat_id}] Set current_flow_id_var to {flow_id}")

            flow = flow_service_bg.get_flow_instance(flow_id)
            if not flow:
                logger.error(f"[Chat {chat_id}] Background task could not find flow {flow_id}.")
                await event_queue.put({"type": "error", "data": {"message": f"Flow {flow_id} not found.", "stage": "setup"}})
                current_flow_id_var.reset(token_cv)
                return
            
            flow_data = flow.flow_data or {}
            logger.debug(f"[Chat {chat_id}] Flow data for context: {str(flow_data)[:200]}...")

            logger.info(f"[Chat {chat_id}] Getting compiled LangGraph from ChatService.")
            compiled_graph = chat_service_bg.compiled_workflow_graph
            logger.info(f"[Chat {chat_id}] Successfully got compiled LangGraph.")

            chat_history_raw = chat.chat_data.get("messages", [])
            
            graph_input_messages = _format_messages_to_langchain(chat_history_raw)
            
            current_user_input_content = ""
            if graph_input_messages and isinstance(graph_input_messages[-1], HumanMessage):
                current_user_input_content = graph_input_messages[-1].content
            else:
                logger.warning(f"[Chat {chat_id}] Could not determine current user input from chat history. Last message: {graph_input_messages[-1] if graph_input_messages else 'No messages'}")
                if initial_user_message_content and not is_edit_flow:
                     current_user_input_content = initial_user_message_content
                elif is_edit_flow and chat_history_raw:
                    last_raw_msg = chat_history_raw[-1]
                    if last_raw_msg.get("role") == "user":
                        current_user_input_content = last_raw_msg.get("content", "")


            graph_input = {
                "messages": graph_input_messages,
                "input": current_user_input_content,
                "flow_context": flow_data.get("graphContextVars", {}),
                "current_flow_id": flow_id,
                "sse_event_queue": event_queue
            }
            
            messages_count_val = len(graph_input["messages"])
            input_len_val = len(graph_input["input"])
            flow_id_val = graph_input["current_flow_id"]
            logger.info(f"[Chat {chat_id}] Prepared graph input: messages_count={messages_count_val}, input_len={input_len_val}, input_content='{current_user_input_content[:50]}...', flow_id={flow_id_val}")

            logger.info(f"[Chat {chat_id}] Invoking compiled_graph.astream_events (version='v2')...")
            
            event_include_names = None

            async for event in compiled_graph.astream_events(graph_input, version="v2", include_names=event_include_names, include_tags=None):
                event_name = event.get("event")
                event_data = event.get("data", {})
                run_name = event.get("name", "unknown_run")

                logger.debug(f"[Chat {chat_id}] Received event: '{event_name}' from '{run_name}', Data keys: {list(event_data.keys())}")
                
                if event_name == "on_chain_end":
                    logger.info(f"[Chat {chat_id}] ğŸš¨ CHAIN END DETECTED: run_name='{run_name}', compiled_graph.name='{compiled_graph.name}'")
                    logger.debug(f"[Chat {chat_id}] ğŸš¨ Output data type: {type(event_data.get('output', 'NO_OUTPUT'))}")
                    if isinstance(event_data.get('output'), dict):
                        output_keys = list(event_data.get('output', {}).keys())
                        logger.debug(f"[Chat {chat_id}] ğŸš¨ Output keys: {output_keys}")
                
                if event_name not in ["on_chat_model_stream"]:
                    logger.debug(f"[Chat {chat_id}] ğŸ“‹ Event details - Name: {event_name}, Run: {run_name}, Data type: {type(event_data)}")

                if event_name == "on_chat_model_stream":
                    chunk = event_data.get("chunk")
                    if chunk and isinstance(chunk, AIMessageChunk) and chunk.content:
                        token = chunk.content
                        logger.debug(f"[Chat {chat_id}] LLM Token from '{run_name}': '{token}'")
                        await event_queue.put({"type": "token", "data": token})
                        final_reply_accumulator += token 
                    elif chunk:
                        logger.warning(f"[Chat {chat_id}] Received on_chat_model_stream chunk from '{run_name}' but no content or not AIMessageChunk. Chunk: {chunk}")

                elif event_name == "on_llm_end":
                    output = event_data.get("output")
                    if output and isinstance(output, AIMessage) and output.content:
                        logger.debug(f"[Chat {chat_id}] LLM End from '{run_name}'. Full output (for verification): '{output.content[:100]}...'")
                        if final_reply_accumulator != output.content and not final_reply_accumulator.endswith(output.content):
                            logger.warning(f"[Chat {chat_id}] Discrepancy between accumulated stream and on_llm_end output from '{run_name}'. Accum: '{final_reply_accumulator[:100]}...', Output: '{output.content[:100]}...'")
                    elif output:
                         logger.warning(f"[Chat {chat_id}] Received on_llm_end from '{run_name}' but no content or not AIMessage. Output: {output}")

                elif event_name == "on_tool_start":
                    tool_name = event_data.get("name")
                    tool_input = event_data.get("input")
                    logger.info(f"[Chat {chat_id}] Tool Start: '{tool_name}' from '{run_name}' with input: {str(tool_input)[:100]}...")
                    await event_queue.put({"type": "tool_start", "data": {"name": tool_name, "input": tool_input}})
                    
                elif event_name == "on_tool_end":
                    tool_name = event_data.get("name")
                    tool_output = event_data.get("output")
                    output_summary = str(tool_output)
                    if len(output_summary) > 200:
                        output_summary = output_summary[:200] + "..."
                    logger.info(f"[Chat {chat_id}] Tool End: '{tool_name}' from '{run_name}' with output: {output_summary}")
                    await event_queue.put({"type": "tool_end", "data": {"name": tool_name, "output_summary": output_summary, "full_output": tool_output}})
                    
                    if tool_name and "sas" in tool_name.lower() and isinstance(tool_output, dict):
                        important_keys = ['sas_step1_generated_tasks', 'sas_step2_generated_task_details', 'dialog_state']
                        if any(key in tool_output for key in important_keys):
                            logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' è¾“å‡ºåŒ…å«é‡è¦çŠ¶æ€ï¼Œè§¦å‘åŒæ­¥")
                            sync_result = _sync_langgraph_state_to_flow(tool_output, flow_id, flow_service_bg)
                            
                            if sync_result and sync_result.get("needs_frontend_update"):
                                logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…·ç»“æŸåå‘é€agent_state_updatedäº‹ä»¶åˆ°å‰ç«¯")
                                await event_queue.put({
                                    "type": "agent_state_updated", 
                                    "data": {
                                        "message": f"Agent state updated by tool '{tool_name}'",
                                        "update_types": sync_result.get("update_types", []),
                                        "flow_id": flow_id,
                                        "agent_state": sync_result.get("updated_agent_state", {}),
                                        "trigger": "tool_end"
                                    }
                                })
                            else:
                                logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' åŒæ­¥æœªäº§ç”Ÿå‰ç«¯æ›´æ–°éœ€æ±‚")
                        else:
                            logger.debug(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' è¾“å‡ºä¸åŒ…å«é‡è¦çŠ¶æ€é”®: {list(tool_output.keys())}")

                elif event_name == "on_chain_end":
                    outputs_from_chain = event_data.get("output", {})
                    logger.info(f"[Chat {chat_id}] ğŸš¨ Chain End: '{run_name}'. Output keys: {list(outputs_from_chain.keys()) if isinstance(outputs_from_chain, dict) else 'Not a dict'}")
                    logger.debug(f"[Chat {chat_id}] ğŸš¨ Chain End output type: {type(outputs_from_chain)}")
                    logger.debug(f"[Chat {chat_id}] ğŸš¨ Chain End output content: {str(outputs_from_chain)[:500]}...")
                    
                    should_sync = False
                    sync_reason = ""
                    
                    if run_name == compiled_graph.name or run_name == "__graph__":
                        should_sync = True
                        sync_reason = "ä¸»å›¾æ‰§è¡Œç»“æŸ"
                        final_state = outputs_from_chain
                        logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶1: ä¸»å›¾ç»“æŸ (run_name: {run_name}, graph_name: {compiled_graph.name})")
                        
                    elif "sas" in run_name.lower() and isinstance(outputs_from_chain, dict):
                        important_keys = [
                            'sas_step1_generated_tasks',
                            'sas_step2_generated_task_details', 
                            'dialog_state',
                            'task_list_accepted',
                            'module_steps_accepted',
                            'current_user_request'
                        ]
                        
                        found_keys = [key for key in important_keys if key in outputs_from_chain]
                        if found_keys:
                            should_sync = True
                            sync_reason = f"SASå­å›¾çŠ¶æ€æ›´æ–° (run_name: {run_name}, found_keys: {found_keys})"
                            final_state = outputs_from_chain
                            logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶2: SASå­å›¾çŠ¶æ€æ›´æ–°")
                            logger.debug(f"[Chat {chat_id}] ğŸ¯ å‘ç°é‡è¦é”®: {found_keys}")
                        else:
                            logger.debug(f"[Chat {chat_id}] ğŸ¯ SASå­å›¾ç»“æŸä½†æ— é‡è¦çŠ¶æ€: {run_name}, keys: {list(outputs_from_chain.keys())}")
                    
                    elif "robot_flow_invoker" in run_name.lower() and isinstance(outputs_from_chain, dict):
                        if "sas_planner_subgraph_state" in outputs_from_chain:
                            should_sync = True
                            sync_reason = f"æœºå™¨äººæµç¨‹èŠ‚ç‚¹å®Œæˆ (run_name: {run_name})"
                            final_state = outputs_from_chain
                            logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶3: æœºå™¨äººæµç¨‹èŠ‚ç‚¹å®Œæˆ")
                        else:
                            logger.debug(f"[Chat {chat_id}] ğŸ¯ æœºå™¨äººæµç¨‹èŠ‚ç‚¹ç»“æŸä½†æ— å­å›¾çŠ¶æ€: {run_name}")
                    
                    if not should_sync:
                        logger.debug(f"[Chat {chat_id}] ğŸ¯ Chain Endä¸æ»¡è¶³åŒæ­¥æ¡ä»¶: run_name='{run_name}', graph_name='{compiled_graph.name}', is_dict={isinstance(outputs_from_chain, dict)}")
                    
                    if should_sync:
                        logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘åŒæ­¥ - åŸå› : {sync_reason}")
                        logger.debug(f"[Chat {chat_id}] ğŸ¯ Final state keys: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not a dict'}. Content: {str(final_state)[:500]}...")
                        
                        if isinstance(final_state, dict):
                            sync_result = _sync_langgraph_state_to_flow(final_state, flow_id, flow_service_bg)
                            
                            if sync_result and sync_result.get("needs_frontend_update"):
                                logger.info(f"[Chat {chat_id}] ğŸ¯ å‘é€agent_state_updatedäº‹ä»¶åˆ°å‰ç«¯")
                                await event_queue.put({
                                    "type": "agent_state_updated", 
                                    "data": {
                                        "message": "Agent state has been updated with new tasks/details",
                                        "update_types": sync_result.get("update_types", []),
                                        "flow_id": flow_id,
                                        "agent_state": sync_result.get("updated_agent_state", {})
                                    }
                                })
                            else:
                                logger.info(f"[Chat {chat_id}] ğŸ¯ åŒæ­¥å®Œæˆä½†æ— éœ€å‰ç«¯æ›´æ–°: sync_result={sync_result}")
                        else:
                            logger.warning(f"[Chat {chat_id}] ğŸ¯ final_stateä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡åŒæ­¥ã€‚ç±»å‹: {type(final_state)}")
                    
                    if run_name == compiled_graph.name or run_name == "__graph__":

                        latest_ai_message_from_state: Optional[str] = None
                        if isinstance(final_state, dict) and "messages" in final_state and isinstance(final_state["messages"], list):
                            
                            for msg_state in reversed(final_state["messages"]):
                                content_candidate: Optional[str] = None
                                is_ai_message = False

                                if isinstance(msg_state, AIMessage):
                                    is_ai_message = True
                                    if hasattr(msg_state, 'content'):
                                        content_candidate = msg_state.content
                                elif isinstance(msg_state, dict) and msg_state.get("type") == "ai":
                                    is_ai_message = True
                                    content_candidate = msg_state.get("content")
                                
                                if is_ai_message:
                                    if content_candidate is not None and isinstance(content_candidate, str) and content_candidate.strip():
                                        latest_ai_message_from_state = content_candidate
                                        logger.info(f"[Chat {chat_id}] Found latest AI message (type: {type(msg_state)}) in final graph state: '{latest_ai_message_from_state[:100]}...'")
                                    else:
                                        logger.warning(f"[Chat {chat_id}] Found AI-like message (type: {type(msg_state)}) in final state, but content is None, not string, or empty/whitespace. Original content: '{str(content_candidate)[:100]}...'")
                                    break
                                if not is_ai_message:
                                    continue
                        
                        if latest_ai_message_from_state:
                            if not final_reply_accumulator:
                                logger.info(f"[Chat {chat_id}] No prior stream. Using AIMessage from final graph state as the reply: '{latest_ai_message_from_state[:100]}...'")
                                final_reply_accumulator = latest_ai_message_from_state
                                await event_queue.put({"type": "token", "data": latest_ai_message_from_state})
                            elif final_reply_accumulator != latest_ai_message_from_state:
                                logger.warning(f"[Chat {chat_id}] Accumulated stream reply ('{final_reply_accumulator[:100]}...') differs from final graph state AIMessage ('{latest_ai_message_from_state[:100]}...').")
                                logger.info(f"[Chat {chat_id}] Overwriting accumulated stream with final AIMessage from graph state for frontend and saving.")
                                final_reply_accumulator = latest_ai_message_from_state
                                await event_queue.put({"type": "token", "data": latest_ai_message_from_state})
                        else:
                            if final_reply_accumulator:
                                logger.info(f"[Chat {chat_id}] Graph ended. No new AIMessage in final state. Using previously accumulated stream as final reply: '{final_reply_accumulator[:100]}...'")
                            else:
                                logger.warning(f"[Chat {chat_id}] Graph ended. No AIMessage in final state and no accumulated stream. Reply will be empty/null if no default is set later.")
                
                elif event_name == "on_chain_error" or event_name == "on_llm_error" or event_name == "on_tool_error":
                    error_content = str(event_data.get("error", "Unknown error"))
                    logger.error(f"[Chat {chat_id}] Error event '{event_name}' from '{run_name}': {error_content}")
                    is_error = True
                    stage = f"error_in_{run_name}"
                    error_obj = event_data.get("error")
                    specific_error_message = str(error_obj) if error_obj else "Details not available"
                    
                    error_data = {"message": f"Error in {run_name}: {specific_error_message}", "stage": stage, "details": str(error_obj)}
                    await event_queue.put({"type": "error", "data": error_data})

    except Exception as e:
        is_error = True
        error_message = f"Error during LangGraph astream_events processing: {str(e)}"
        logger.error(f"[Chat {chat_id}] {error_message}", exc_info=True)
        error_data = {"message": error_message, "stage": "graph_execution"}
        try:
            await event_queue.put({"type": "error", "data": error_data})
        except asyncio.QueueFull:
            logger.error(f"[Chat {chat_id}] Failed to put error message in full queue after main exception.")
        except Exception as qe:
            logger.error(f"[Chat {chat_id}] Failed to put error message in queue after main exception: {qe}")

    finally:
        if token_cv is not None: 
            current_flow_id_var.reset(token_cv)
            logger.info(f"[Chat {chat_id}] Reset current_flow_id context variable in finally block.")
        else:
            logger.warning(f"[Chat {chat_id}] current_flow_id_var might not have been set or was already reset, skipping reset in finally.")

        session_should_end = False
        if isinstance(final_state, dict) and final_state.get("output") == "__end__":
            session_should_end = True
        elif isinstance(final_state, str) and final_state == "__end__":
            session_should_end = True

        if session_should_end and not final_reply_accumulator and not is_error:
            default_goodbye_message = "å¥½çš„ï¼Œå†è§ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥å†æ¬¡è”ç³»æˆ‘ã€‚"
            logger.info(f"[Chat {chat_id}] Session is ending and no AI reply was generated. Using default goodbye: '{default_goodbye_message}'")
            final_reply_accumulator = default_goodbye_message
            try:
                await event_queue.put({"type": "token", "data": default_goodbye_message})
                logger.info(f"[Chat {chat_id}] Sent default goodbye message to event queue.")
            except asyncio.QueueFull:
                logger.error(f"[Chat {chat_id}] Failed to put default goodbye message in full queue.")
            except Exception as qe_goodbye:
                logger.error(f"[Chat {chat_id}] Failed to put default goodbye message in queue: {qe_goodbye}")

        if not is_error and final_reply_accumulator:
            try:
                with get_db_context() as db_session_final:
                    chat_service_final = ChatService(db_session_final)
                    logger.info(f"[Chat {chat_id}] Saving AI assistant reply to DB: {final_reply_accumulator[:100]}...")
                    chat_service_final.add_message_to_chat(
                        chat_id=chat_id,
                        role="assistant",
                        content=final_reply_accumulator
                    )
                    logger.info(f"[Chat {chat_id}] AI assistant reply saved to DB successfully.")
            except Exception as save_err:
                logger.error(f"[Chat {chat_id}] Failed to save AI reply to DB: {save_err}", exc_info=True)
        elif is_error:
            logger.warning(f"[Chat {chat_id}] Skipping AI reply save due to an error during processing. Error: {error_data}")
        else:
            logger.info(f"[Chat {chat_id}] Skipping save because final reply was empty or null. Accumulator content: '{final_reply_accumulator}'")
        
        try:
            logger.info(f"[Chat {chat_id}] Putting STREAM_END_SENTINEL into queue.")
            await event_queue.put(STREAM_END_SENTINEL)
            logger.info(f"[Chat {chat_id}] Stream end sentinel sent.")
        except asyncio.QueueFull:
            logger.error(f"[Chat {chat_id}] Failed to put STREAM_END_SENTINEL in full queue.")
        except Exception as qe:
            logger.error(f"[Chat {chat_id}] Failed to put STREAM_END_SENTINEL in queue: {qe}")
        
        if chat_id in active_chat_queues and active_chat_queues[chat_id] is not event_queue:
            try:
                current_queue = active_chat_queues[chat_id]
                logger.info(f"[Chat {chat_id}] Found active queue during cleanup. Queue size: {current_queue.qsize()}")
                
                has_end_sentinel = False
                temp_items = []
                while not current_queue.empty():
                    try:
                        item = current_queue.get_nowait()
                        temp_items.append(item)
                        if item is STREAM_END_SENTINEL:
                            has_end_sentinel = True
                            logger.info(f"[Chat {chat_id}] Found existing STREAM_END_SENTINEL in queue")
                        current_queue.task_done()
                    except asyncio.QueueEmpty:
                        break
                
                for item in temp_items:
                    await current_queue.put(item)
                
                if not has_end_sentinel:
                    logger.info(f"[Chat {chat_id}] No STREAM_END_SENTINEL found in queue, adding one now")
                    await current_queue.put(STREAM_END_SENTINEL)
                    logger.info(f"[Chat {chat_id}] STREAM_END_SENTINEL added to queue during cleanup")
                    
            except asyncio.QueueFull:
                logger.error(f"[Chat {chat_id}] Failed to add STREAM_END_SENTINEL during cleanup - queue is full")
            except Exception as cleanup_err:
                logger.error(f"[Chat {chat_id}] Error during queue cleanup: {cleanup_err}", exc_info=True)
        else:
            logger.info(f"[Chat {chat_id}] No active queue found during cleanup")
            
        logger.info(f"[Chat {chat_id}] Background task (is_edit_flow: {is_edit_flow}) final cleanup completed.")

def _sync_langgraph_state_to_flow(final_state, flow_id, flow_service_bg):
    try:
        logger.info(f"[Flow {flow_id}] ğŸ¯ å¼€å§‹åŒæ­¥LangGraphçŠ¶æ€åˆ°Flow agent_state...")
        logger.debug(f"[Flow {flow_id}] ğŸ¯ final_stateé”®å€¼: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not a dict'}")
        logger.debug(f"[Flow {flow_id}] ğŸ¯ final_stateå†…å®¹æ‘˜è¦: {str(final_state)[:1000]}...")
        
        flow = flow_service_bg.get_flow_instance(flow_id)
        if not flow:
            logger.error(f"[Flow {flow_id}] æ— æ³•æ‰¾åˆ°Flowï¼ŒåŒæ­¥å¤±è´¥")
            return None
        
        current_agent_state = flow.agent_state or {}
        logger.debug(f"[Flow {flow_id}] ğŸ¯ å½“å‰agent_stateé”®å€¼: {list(current_agent_state.keys())}")
        
        needs_sync = False
        sync_updates = {}
        update_types = []
        
        important_fields = [
            'sas_step1_generated_tasks',
            'sas_step2_generated_task_details', 
            'task_list_accepted',
            'module_steps_accepted',
            'dialog_state',
            'subgraph_completion_status',
            'current_user_request',
            'revision_iteration',
            'input_processed'
        ]
        
        for field in important_fields:
            if field in final_state:
                current_value = current_agent_state.get(field)
                new_value = final_state[field]

                # <<< START DEBUG LOGGING >>>
                if field == 'dialog_state' or field == 'sas_step1_generated_tasks':
                    logger.info(f"[SYNC_DEBUG] Comparing field: {field}")
                    logger.info(f"[SYNC_DEBUG]   current_value ('{type(current_value)}'): {str(current_value)[:300]}")
                    logger.info(f"[SYNC_DEBUG]   new_value     ('{type(new_value)}'): {str(new_value)[:300]}")
                    is_different = current_value != new_value
                    logger.info(f"[SYNC_DEBUG]   Comparison result (current_value != new_value): {is_different}")
                # <<< END DEBUG LOGGING >>>

                if current_value != new_value:
                    current_value_str = str(current_value)[:50] + ('...' if len(str(current_value)) > 50 else '')
                    new_value_str = str(new_value)[:50] + ('...' if len(str(new_value)) > 50 else '')
                    logger.info(f"[Flow {flow_id}] ğŸ¯ æ£€æµ‹åˆ°{field}å˜åŒ–: {current_value_str} -> {new_value_str}")
                    sync_updates[field] = new_value
                    update_types.append(field)
                    needs_sync = True
        
        sas_subgraph_state = final_state.get('sas_planner_subgraph_state')
        if sas_subgraph_state and isinstance(sas_subgraph_state, dict):
            logger.info(f"[Flow {flow_id}] ğŸ¯ å‘ç°SASå­å›¾çŠ¶æ€ï¼Œå¼€å§‹æå–æ•°æ®...")
            logger.debug(f"[Flow {flow_id}] ğŸ¯ SASå­å›¾çŠ¶æ€é”®å€¼: {list(sas_subgraph_state.keys())}")
            
            sas_important_fields = [
                'sas_step1_generated_tasks',
                'sas_step2_generated_task_details',
                'sas_step2_module_steps',
                'task_list_accepted',
                'module_steps_accepted',
                'dialog_state',
                'subgraph_completion_status',
                'current_user_request',
                'revision_iteration'
            ]
            
            for field in sas_important_fields:
                if field in sas_subgraph_state:
                    current_value = current_agent_state.get(field)
                    new_value = sas_subgraph_state[field]
                    if current_value != new_value:
                        current_value_str_sas = str(current_value)[:50] + ('...' if len(str(current_value)) > 50 else '')
                        new_value_str_sas = str(new_value)[:50] + ('...' if len(str(new_value)) > 50 else '')
                        logger.info(f"[Flow {flow_id}] ğŸ¯ ä»SASå­å›¾æ£€æµ‹åˆ°{field}å˜åŒ–: {current_value_str_sas} -> {new_value_str_sas}")
                        sync_updates[field] = new_value
                        update_types.append(f"sas_{field}")
                        needs_sync = True
            
            sas_tasks = sas_subgraph_state.get('sas_step1_generated_tasks')
            if sas_tasks:
                logger.info(f"[Flow {flow_id}] ğŸ¯ SASå­å›¾åŒ…å« {len(sas_tasks)} ä¸ªä»»åŠ¡:")
                for i, task in enumerate(sas_tasks):
                    if isinstance(task, dict):
                        logger.debug(f"[Flow {flow_id}] ğŸ¯   ä»»åŠ¡{i+1}: {task.get('name', 'Unknown')} (ç±»å‹: {task.get('type', 'Unknown')})")
                    else:
                        logger.debug(f"[Flow {flow_id}] ğŸ¯   ä»»åŠ¡{i+1}: {task}")
            
            sas_details = sas_subgraph_state.get('sas_step2_generated_task_details')
            if sas_details:
                logger.info(f"[Flow {flow_id}] SASå­å›¾åŒ…å«ä»»åŠ¡è¯¦æƒ…: {len(sas_details)} é¡¹")
                for task_key, details in sas_details.items():
                    if isinstance(details, dict) and 'details' in details:
                        detail_count = len(details['details']) if isinstance(details['details'], list) else 1
                        logger.debug(f"[Flow {flow_id}] ğŸ¯   {task_key}: {detail_count} ä¸ªè¯¦æƒ…")
        
        has_task_data = (
            sync_updates.get('sas_step1_generated_tasks') or 
            current_agent_state.get('sas_step1_generated_tasks') or
            (sas_subgraph_state and sas_subgraph_state.get('sas_step1_generated_tasks'))
        )
        
        has_detail_data = (
            sync_updates.get('sas_step2_generated_task_details') or
            current_agent_state.get('sas_step2_generated_task_details') or
            (sas_subgraph_state and sas_subgraph_state.get('sas_step2_generated_task_details'))
        )
        
        needs_frontend_update = has_task_data or has_detail_data
        if needs_frontend_update:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ£€æµ‹åˆ°ä»»åŠ¡æ•°æ®ï¼Œéœ€è¦å‰ç«¯èŠ‚ç‚¹æ›´æ–°")
            update_types.append("frontend_nodes")
            needs_sync = True
        
        if needs_sync:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ‰§è¡ŒçŠ¶æ€åŒæ­¥ï¼Œæ›´æ–° {len(sync_updates)} ä¸ªå­—æ®µ:")
            for key, value in sync_updates.items():
                logger.debug(f"[Flow {flow_id}] ğŸ¯   {key}: {str(value)[:200]}...")
            
            current_agent_state.update(sync_updates)
            flow.agent_state = current_agent_state
            
            result = {
                "needs_frontend_update": needs_frontend_update,
                "update_types": update_types,
                "updated_agent_state": current_agent_state,
                "sync_updates": sync_updates
            }
            
            logger.info(f"[Flow {flow_id}] ğŸ¯ çŠ¶æ€åŒæ­¥å®Œæˆï¼Œéœ€è¦å‰ç«¯æ›´æ–°: {needs_frontend_update}")
            return result
        else:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦åŒæ­¥çš„çŠ¶æ€å˜åŒ–")
            return None
    
    except Exception as e:
        logger.error(f"[Flow {flow_id}] ğŸ¯ çŠ¶æ€åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return None 