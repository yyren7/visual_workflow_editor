from typing import List, Dict, Any, Optional, AsyncGenerator
from fastapi import APIRouter, Depends, HTTPException, Query, status, Request, Response, BackgroundTasks
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from sqlalchemy.orm import Session
import logging
import json
import asyncio # ç¡®ä¿å¯¼å…¥ asyncio
import time  # æ·»åŠ æ—¶é—´æ¨¡å—å¯¼å…¥
from datetime import datetime # <--- ä¿®æ”¹æ­¤è¡Œ
from collections import defaultdict # å¯¼å…¥ defaultdict
from backend.langgraphchat.context import current_flow_id_var # <--- Import context variable

from backend.app import schemas
from database.connection import get_db, get_db_context
from backend.app.utils import get_current_user, verify_flow_ownership
from backend.app.services.chat_service import ChatService
from backend.app.services.flow_service import FlowService
from database.models import Flow
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, AIMessageChunk

logger = logging.getLogger(__name__)

# --- æ–°å¢ï¼šç”¨äºå­˜å‚¨æ´»åŠ¨äº‹ä»¶æµçš„å†…å­˜é˜Ÿåˆ— --- 
# Key: chat_id, Value: asyncio.Queue
# æ³¨æ„ï¼šç®€å•å†…å­˜å®ç°ï¼Œä¸é€‚ç”¨äºå¤šè¿›ç¨‹/å¤šå®ä¾‹éƒ¨ç½²
active_chat_queues: Dict[str, asyncio.Queue] = defaultdict(asyncio.Queue)
# ç”¨äºè¿½è¸ªæ¯ä¸ªchatçš„SSEè¿æ¥æ•°
active_sse_connections: Dict[str, int] = defaultdict(int)
# ç”¨äºé€šçŸ¥ GET è¯·æ±‚æµå·²ç»“æŸçš„æ ‡è®°
STREAM_END_SENTINEL = {"type": "stream_end", "data": {"message": "Stream finished or no stream generated."}}
# é˜Ÿåˆ—æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢å†…å­˜æ— é™å¢é•¿
MAX_QUEUE_SIZE = 100 
# --- ç»“æŸæ–°å¢ ---

router = APIRouter(
    prefix="/chats",
    tags=["chats"],
    responses={404: {"description": "Not found"}},
)

# --- è¾…åŠ©å‡½æ•°ï¼šå°†æ•°æ®åº“æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º Langchain æ ¼å¼ ---
def _format_messages_to_langchain(messages: List[Dict]) -> List[BaseMessage]:
    """å°†åŒ…å« 'role' å’Œ 'content' çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º Langchain BaseMessage åˆ—è¡¨ã€‚"""
    langchain_messages = []
    for message in messages:
        role = message.get("role")
        content = message.get("content", "")
        if role == "user":
            langchain_messages.append(HumanMessage(content=content))
        elif role == "assistant":
            langchain_messages.append(AIMessage(content=content))
        # å¯ä»¥é€‰æ‹©æ€§åœ°å¤„ç†å…¶ä»– roleï¼Œä¾‹å¦‚ 'system'
        # else:
        #     logger.warning(f"Unknown message role '{role}' encountered during formatting.")
    return langchain_messages

@router.post("/", response_model=schemas.Chat)
async def create_chat(
    chat: schemas.ChatCreate, 
    request: Request,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    åˆ›å»ºæ–°çš„èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½ä¸ºè‡ªå·±çš„æµç¨‹å›¾åˆ›å»ºèŠå¤©
    """
    print(f"æˆåŠŸè¿›å…¥ create_chat å‡½æ•°: {request.method} {request.url.path}")
    logger.info(f"æˆåŠŸè¿›å…¥ create_chat å‡½æ•°: {request.method} {request.url.path}")

    try:
        # éªŒè¯æµç¨‹å›¾å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
        logger.info(f"éªŒè¯ flow ownership: {chat.flow_id}")
        verified_flow = verify_flow_ownership(chat.flow_id, current_user, db)
        logger.info(f"Flow ownership éªŒè¯é€šè¿‡ for flow: {verified_flow.id}")

        # åˆ›å»ºèŠå¤©
        logger.info(f"è°ƒç”¨ chat_service.create_chat")
        chat_service = ChatService(db)
        db_chat = chat_service.create_chat(
            flow_id=chat.flow_id,
            name=chat.name,
            chat_data=chat.chat_data
        )
        logger.info(f"chat_service.create_chat è¿”å›: {db_chat}")

        if not db_chat:
            logger.error("chat_service.create_chat æœªæˆåŠŸåˆ›å»ºèŠå¤©ï¼Œå¼•å‘ 400")
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ— æ³•åˆ›å»ºèŠå¤©"
            )

        # --- Update Flow's last_interacted_chat_id ---
        if verified_flow: # Ensure we have the flow object
            try:
                verified_flow.last_interacted_chat_id = db_chat.id
                db.commit()
                logger.info(f"æ›´æ–° Flow {verified_flow.id} çš„ last_interacted_chat_id ä¸º {db_chat.id}")
            except Exception as update_err:
                logger.error(f"æ›´æ–° Flow last_interacted_chat_id å¤±è´¥ (Flow: {verified_flow.id}, Chat: {db_chat.id}): {update_err}", exc_info=True)
                db.rollback() # Rollback only the failed update
        else:
             logger.warning(f"æ— æ³•æ›´æ–° Flow çš„ last_interacted_chat_idï¼Œå› ä¸ºåœ¨åˆ›å»ºèŠå¤©åæœªèƒ½è·å– Flow å¯¹è±¡ (Flow ID: {chat.flow_id})")
        # --- End Update ---

        logger.info(f"æˆåŠŸåˆ›å»ºèŠå¤©ï¼Œè¿”å› chat å¯¹è±¡ ID: {db_chat.id}")
        return db_chat
    except HTTPException as http_exc:
        logger.error(f"å¤„ç† create_chat æ—¶å‘ç”Ÿ HTTPException: {http_exc.status_code} - {http_exc.detail}")
        raise http_exc
    except Exception as e:
        logger.error(f"å¤„ç† create_chat æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"åˆ›å»ºèŠå¤©æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"
        )


@router.get("/{chat_id}", response_model=schemas.Chat)
async def get_chat(
    chat_id: str, 
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    è·å–èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½è®¿é—®è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    return chat


@router.get("/flow/{flow_id}", response_model=List[schemas.Chat])
async def get_flow_chats(
    flow_id: str, 
    skip: int = Query(0, ge=0), 
    limit: int = Query(100, le=1000),
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    è·å–æµç¨‹å›¾çš„æ‰€æœ‰èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½è®¿é—®è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(flow_id, current_user, db)
    
    # è·å–èŠå¤©åˆ—è¡¨
    chat_service = ChatService(db)
    chats = chat_service.get_chats_for_flow(flow_id, skip, limit)
    
    return chats


@router.put("/{chat_id}", response_model=schemas.Chat)
async def update_chat(
    chat_id: str,
    chat_update: schemas.ChatUpdate,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    æ›´æ–°èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½æ›´æ–°è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    # æ›´æ–°èŠå¤©
    updated_chat = chat_service.update_chat(
        chat_id=chat_id,
        name=chat_update.name,
        chat_data=chat_update.chat_data
    )
    
    if not updated_chat:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æ›´æ–°èŠå¤©å¤±è´¥"
        )
    
    return updated_chat


@router.put("/{chat_id}/messages/{message_timestamp}", status_code=status.HTTP_202_ACCEPTED)
async def edit_user_message(
    chat_id: str,
    message_timestamp: str, 
    edit_data: schemas.ChatMessageEdit, 
    db: Session = Depends(get_db),
    current_user: schemas.User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """
    ç¼–è¾‘ç”¨æˆ·æ¶ˆæ¯, åˆ é™¤æ­¤æ¶ˆæ¯ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯, å¹¶ä»¥æ–°å†…å®¹é‡æ–°ç”Ÿæˆç”¨æˆ·æ¶ˆæ¯ã€‚
    ç„¶ååƒæ–°æ¶ˆæ¯ä¸€æ ·è§¦å‘ LangGraph å·¥ä½œæµã€‚
    å¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½æ“ä½œè‡ªå·±æµç¨‹å›¾çš„èŠå¤©ã€‚
    ç«‹å³è¿”å› 202 Acceptedï¼Œå®¢æˆ·ç«¯éœ€è¦éšåè¿æ¥ GET /{chat_id}/events è·å–äº‹ä»¶ã€‚
    """
    logger.info(f"Attempting to edit message {message_timestamp} in chat {chat_id} and trigger workflow.")
    chat_service = ChatService(db)
    chat_before_edit = chat_service.get_chat(chat_id)

    if not chat_before_edit:
        logger.error(f"Chat {chat_id} not found for editing message.")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    verify_flow_ownership(chat_before_edit.flow_id, current_user, db)
    logger.debug(f"Ownership verified for flow {chat_before_edit.flow_id}")

    # åœ¨è°ƒç”¨æœåŠ¡å±‚ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å­˜åœ¨
    message_found = False
    if chat_before_edit.chat_data and "messages" in chat_before_edit.chat_data:
        messages = chat_before_edit.chat_data.get("messages", [])
        if isinstance(messages, list): # ç¡®ä¿ messages æ˜¯åˆ—è¡¨
            for msg in messages:
                if isinstance(msg, dict) and msg.get("timestamp") == message_timestamp and msg.get("role") == "user":
                    message_found = True
                    break
    
    if not message_found:
        logger.error(f"User message with timestamp {message_timestamp} not found in chat {chat_id} before calling service.")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"è¦ç¼–è¾‘çš„æ¶ˆæ¯ä¸å­˜åœ¨æˆ–æ—¶é—´æˆ³ä¸åŒ¹é… (ts: {message_timestamp})"
        )

    updated_chat = chat_service.edit_user_message_and_truncate(
        chat_id=chat_id,
        message_timestamp=message_timestamp,
        new_content=edit_data.new_content
    )

    if not updated_chat:
        logger.error(f"Failed to edit message {message_timestamp} in chat {chat_id} via service.")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, # æ”¹ä¸º500ï¼Œå› ä¸ºæ­¤æ—¶æ›´å¯èƒ½æ˜¯å†…éƒ¨é—®é¢˜
            detail="ç¼–è¾‘æ¶ˆæ¯æ—¶å‘ç”Ÿå†…éƒ¨æœåŠ¡å™¨é”™è¯¯"
        )
    
    logger.info(f"Successfully edited message {message_timestamp} in chat {chat_id}. DB state updated.")

    # --- è§¦å‘åå°äº‹ä»¶å¤„ç† ---
    event_queue = asyncio.Queue(maxsize=MAX_QUEUE_SIZE) 
    active_chat_queues[chat_id] = event_queue
    logger.info(f"ä¸º chat {chat_id} (after edit) åˆ›å»º/è®¾ç½®äº†æ–°çš„äº‹ä»¶é˜Ÿåˆ—")

    # The initial_user_message_content is not strictly needed by _process_and_publish_chat_events
    # when is_edit_flow is True, as it will read the latest from DB.
    background_tasks.add_task(
        _process_and_publish_chat_events, 
        chat_id, 
        initial_user_message_content=None, # Content is already in DB
        event_queue=event_queue,
        is_edit_flow=True
    )
    logger.info(f"å·²ä¸º chat {chat_id} (after edit) å¯åŠ¨åå°äº‹ä»¶å¤„ç†ä»»åŠ¡")

    return Response(status_code=status.HTTP_202_ACCEPTED)


@router.post("/{chat_id}/messages")
async def add_message(
    chat_id: str,
    message: schemas.ChatAddMessage,
    db: Session = Depends(get_db),
    current_user: schemas.User = Depends(get_current_user),
    background_tasks: BackgroundTasks = BackgroundTasks()
) -> Response:
    """
    å‘èŠå¤©æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œè§¦å‘åå°å¤„ç†æµç¨‹ã€‚
    æ”¯æŒè™šæ‹ŸChat IDï¼ˆä½¿ç”¨flow_idä½œä¸ºchat_idï¼‰ã€‚
    ç«‹å³è¿”å› 202 Acceptedï¼Œå®¢æˆ·ç«¯éœ€è¦éšåè¿æ¥ GET /{chat_id}/events è·å–äº‹ä»¶ã€‚
    """
    chat_service = ChatService(db)
    flow_service = FlowService(db)
    
    # é¦–å…ˆå°è¯•è·å–å¸¸è§„èŠå¤©
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        # æ£€æŸ¥chat_idæ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„LangGraphè™šæ‹ŸChat ID
        logger.info(f"Chat {chat_id} not found, checking if it's a virtual LangGraph chat ID...")
        
        # ä»chat_idä¸­è§£æflow_idã€task_indexã€detail_index
        # æ”¯æŒæ ¼å¼ï¼šflow_id, flow_id_task_X, flow_id_task_X_detail_Y
        flow_id = chat_id.split('_task_')[0].split('_detail_')[0]
        task_index = None
        detail_index = None
        
        # è§£ætask_index
        if '_task_' in chat_id:
            task_part = chat_id.split('_task_')[1]
            if '_detail_' in task_part:
                task_index = int(task_part.split('_detail_')[0])
                detail_index = int(task_part.split('_detail_')[1])
            else:
                task_index = int(task_part)
        
        # å°è¯•éªŒè¯è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„flow_id
        try:
            flow = verify_flow_ownership(flow_id, current_user, db)
            if flow:
                logger.info(f"Virtual LangGraph chat detected: {chat_id} -> flow_id: {flow_id}, task: {task_index}, detail: {detail_index}")
                
                # ä¸ºè™šæ‹ŸèŠå¤©åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„èŠå¤©ä¼šè¯
                # è¿™æ ·å¯ä»¥å¤ç”¨ç°æœ‰çš„èŠå¤©å¤„ç†é€»è¾‘
                if task_index is not None and detail_index is not None:
                    virtual_chat_name = f"Virtual Detail Chat - {flow.name} Task {task_index + 1} Detail {detail_index + 1}"
                elif task_index is not None:
                    virtual_chat_name = f"Virtual Task Chat - {flow.name} Task {task_index + 1}"
                else:
                    virtual_chat_name = f"Virtual LangGraph Chat - {flow.name}"
                
                virtual_chat = chat_service.create_chat(
                    flow_id=flow_id,  # ä½¿ç”¨è§£æå‡ºçš„flow_id
                    name=virtual_chat_name,
                    chat_data={
                        "messages": [], 
                        "is_virtual_langgraph_chat": True,
                        "virtual_chat_id": chat_id,  # ä¿å­˜åŸå§‹çš„è™šæ‹Ÿchat_id
                        "task_index": task_index,
                        "detail_index": detail_index
                    }
                )
                
                if virtual_chat:
                    logger.info(f"Created virtual chat {virtual_chat.id} for LangGraph chat_id {chat_id}")
                    # ä½¿ç”¨æ–°åˆ›å»ºçš„è™šæ‹ŸèŠå¤©
                    chat = virtual_chat
                    # é‡è¦ï¼šä½¿ç”¨å®é™…åˆ›å»ºçš„chat IDï¼Œè€Œä¸æ˜¯è™šæ‹Ÿchat ID
                    actual_chat_id = virtual_chat.id
                else:
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail="æ— æ³•åˆ›å»ºè™šæ‹ŸèŠå¤©ä¼šè¯"
                    )
            else:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨ä¸”ä¸æ˜¯æœ‰æ•ˆçš„æµç¨‹å›¾ID")
        except HTTPException as he:
            # å¦‚æœéªŒè¯å¤±è´¥ï¼Œé‡æ–°æŠ›å‡ºåŸå§‹çš„404é”™è¯¯
            if he.status_code == 404:
                raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨")
            else:
                raise he
        except Exception as e:
            logger.error(f"Error checking virtual LangGraph chat ID {chat_id}: {e}")
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="èŠå¤©ä¸å­˜åœ¨")
    else:
        # å¸¸è§„èŠå¤©æµç¨‹
        actual_chat_id = chat_id
    
    # éªŒè¯æµç¨‹å›¾å½’å±ï¼ˆå¸¸è§„èŠå¤©å’Œè™šæ‹ŸèŠå¤©éƒ½éœ€è¦ï¼‰
    verified_flow = verify_flow_ownership(chat.flow_id, current_user, db)
    logger.info(f"Flow ownership éªŒè¯é€šè¿‡ for flow: {verified_flow.id} linked to chat: {chat.id}")
    
    if message.role != 'user':
         logger.warning(f"Received message with role '{message.role}' in add_message. Processing as user message.")
         
    # å¯¹äºè™šæ‹ŸèŠå¤©ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åŸå§‹çš„chat_idï¼ˆå³flow_idï¼‰ä½œä¸ºäº‹ä»¶é˜Ÿåˆ—çš„key
    # è¿™æ ·å‰ç«¯å°±èƒ½ç”¨flow_idæ¥ç›‘å¬äº‹ä»¶
    event_queue_key = chat_id  # ä½¿ç”¨åŸå§‹è¯·æ±‚çš„chat_id
    actual_processing_chat_id = chat.id  # ä½¿ç”¨å®é™…çš„chatè®°å½•IDè¿›è¡Œå¤„ç†
    
    event_queue = asyncio.Queue(maxsize=MAX_QUEUE_SIZE) 
    active_chat_queues[event_queue_key] = event_queue  # ä½¿ç”¨åŸå§‹chat_idä½œä¸ºkey
    logger.info(f"ä¸º chat {event_queue_key} (actual: {actual_processing_chat_id}) åˆ›å»º/è®¾ç½®äº†æ–°çš„äº‹ä»¶é˜Ÿåˆ—")

    background_tasks.add_task(
        _process_and_publish_chat_events, 
        actual_processing_chat_id,  # ä¼ é€’å®é™…çš„chat IDç»™åå°ä»»åŠ¡
        initial_user_message_content=message.content, 
        event_queue=event_queue,
        is_edit_flow=False,
        client_message_id=message.client_message_id
    )
    logger.info(f"å·²ä¸º chat {event_queue_key} (processing: {actual_processing_chat_id}) å¯åŠ¨åå°äº‹ä»¶å¤„ç†ä»»åŠ¡ (new message, client_id: {message.client_message_id})")
    
    return Response(status_code=status.HTTP_202_ACCEPTED)


@router.get("/{chat_id}/events")
async def get_chat_events(chat_id: str, request: Request):
    """
    ç”¨äºå®¢æˆ·ç«¯é€šè¿‡ EventSource è¿æ¥ä»¥æ¥æ”¶èŠå¤©äº‹ä»¶ã€‚
    æ”¯æŒè™šæ‹ŸChat IDï¼ˆflow_idä½œä¸ºchat_idï¼‰ã€‚
    """
    logger.info(f"æ”¶åˆ°å¯¹ chat {chat_id} äº‹ä»¶æµçš„ GET è¯·æ±‚ from IP: {request.client.host if request.client else 'unknown'}")

    if chat_id not in active_chat_queues:
        logger.warning(f"è¯·æ±‚ chat {chat_id} çš„äº‹ä»¶æµï¼Œä½†é˜Ÿåˆ—ä¸å­˜åœ¨æˆ–å·²æ¸…ç†")
        # It's possible the client is trying to connect after the stream has ended and queue cleaned up.
        # Return a specific SSE event indicating this rather than a 404, so client can handle gracefully.
        async def immediate_end_stream():
            yield {
                "event": "stream_end", # Or a custom "already_closed" event type
                "data": json.dumps({"message": f"No active event stream for chat {chat_id}. It may have already finished or was never started."})
            }
            logger.info(f"Sent immediate stream_end for non-existent/cleaned queue {chat_id}")
        return EventSourceResponse(immediate_end_stream())

    event_queue = active_chat_queues[chat_id]
    logger.info(f"æ‰¾åˆ° chat {chat_id} çš„äº‹ä»¶é˜Ÿåˆ—ï¼Œå‡†å¤‡å‘é€ SSE äº‹ä»¶")

    async def sse_event_sender():
        # è¿½è¸ªè¿æ¥æ•°
        active_sse_connections[chat_id] += 1
        connection_count = active_sse_connections[chat_id]
        logger.info(f"ğŸ”´ Starting SSE event sender #{connection_count} for chat {chat_id}")
        
        # å¦‚æœå·²ç»æœ‰è¿æ¥ï¼Œå‘å‡ºè­¦å‘Š
        if connection_count > 1:
            logger.warning(f"ğŸ”´ WARNING: Multiple SSE connections detected for chat {chat_id}! Count: {connection_count}")
        
        client_disconnected = False
        event_data = None # Initialize event_data

        async def check_disconnect():
            nonlocal client_disconnected
            try:
                # æ›´å¯é çš„æ–­å¼€æ£€æµ‹ï¼šæ£€æŸ¥è¿æ¥çŠ¶æ€
                is_disconnected = await request.is_disconnected()
                if is_disconnected:
                    client_disconnected = True
                    logger.info(f"SSE client for chat {chat_id} disconnected (detected via request.is_disconnected()).")
                    return True
                return False
            except Exception as e:
                # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œå‡è®¾è¿æ¥ä»ç„¶æ´»è·ƒï¼Œä½†è®°å½•è­¦å‘Š
                logger.warning(f"Could not check client disconnect status for chat {chat_id}: {e}")
                return False

        try:
            # å‘é€åˆå§‹çš„å¿ƒè·³/è¿æ¥ç¡®è®¤äº‹ä»¶
            logger.debug(f"SSE sender for {chat_id}: Sending initial ping.")
            yield {
                "event": "ping",
                "data": json.dumps({"timestamp": datetime.utcnow().isoformat(), "message": "SSE connection established"})
            }

            while not client_disconnected:
                event_data = None # Reset event_data at the start of each iteration
                try:
                    # ç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œæ›´å¿«æ£€æµ‹æ–­å¼€
                    logger.debug(f"[SSE {chat_id}] Waiting for event from queue...")
                    event_data = await asyncio.wait_for(event_queue.get(), timeout=0.5) 
                    logger.debug(f"[SSE {chat_id}] Got event: {str(event_data)[:100]}")
                except asyncio.TimeoutError:
                    # å…ˆæ£€æŸ¥æ–­å¼€çŠ¶æ€
                    is_disconnected = await check_disconnect()
                    if is_disconnected or client_disconnected:
                        logger.info(f"[SSE {chat_id}] Client disconnected after timeout. Breaking loop.")
                        break 
                    
                    # å‡å°‘pingé¢‘ç‡ï¼Œæ¯5ç§’å‘é€ä¸€æ¬¡
                    current_time = time.time()
                    if not hasattr(check_disconnect, '_last_ping_time'):
                        check_disconnect._last_ping_time = 0
                    
                    time_since_ping = current_time - check_disconnect._last_ping_time
                    if time_since_ping < 5.0:  # 5ç§’å†…ä¸é‡å¤å‘é€ping
                        continue
                    
                    # Send ping if not disconnected
                    logger.debug(f"[SSE {chat_id}] Client still connected. Sending ping.")
                    try:
                        yield {
                            "event": "ping",
                            "data": json.dumps({"timestamp": datetime.utcnow().isoformat(), "message": "keep-alive"})
                        }
                        check_disconnect._last_ping_time = current_time
                    except Exception as ping_err:
                        logger.error(f"[SSE {chat_id}] Error sending ping: {ping_err}", exc_info=True)
                        # pingå‘é€å¤±è´¥é€šå¸¸æ„å‘³ç€å®¢æˆ·ç«¯å·²æ–­å¼€
                        client_disconnected = True
                        break
                    continue
                except asyncio.CancelledError:
                    logger.info(f"SSE event sender for chat {chat_id} was cancelled (likely client disconnect or task shutdown).")
                    client_disconnected = True # Ensure flag is set
                    break # Exit loop
                except Exception as e_get:
                    logger.error(f"[SSE {chat_id}] Error getting event from queue: {e_get}", exc_info=True)
                    await check_disconnect() # Check if this error caused disconnect
                    if client_disconnected:
                        break
                    # If still connected, report error and continue or break based on severity?
                    # For now, let's try to send an error and break to be safe.
                    try:
                        yield {"event": "error", "data": json.dumps({"message": f"Error fetching event from queue: {str(e_get)}", "stage": "sse_queue_read_error"})}
                    except Exception as send_q_err:
                        logger.error(f"[SSE {chat_id}] Failed to send queue read error to client: {send_q_err}")
                    client_disconnected = True # Assume critical error, stop processing
                    break

                if event_data is None: # Should not happen if loop logic is correct, but as a safeguard
                    logger.warning(f"[SSE {chat_id}] event_data is None after queue.get() succeeded without timeout/exception. This is unexpected. Skipping.")
                    continue
                
                if event_data is STREAM_END_SENTINEL:
                    logger.info(f"[SSE {chat_id}] Received stream end sentinel. Sending final event and closing.")
                    try:
                        yield {
                            "event": STREAM_END_SENTINEL.get("type", "stream_end"),
                            "data": json.dumps(STREAM_END_SENTINEL.get("data", {}))
                        }
                        event_queue.task_done()
                    except Exception as send_final_err:
                        logger.error(f"[SSE {chat_id}] Error sending stream_end sentinel: {send_final_err}", exc_info=True)
                    
                    # å¼ºåˆ¶æ ‡è®°ä¸ºæ–­å¼€è¿æ¥
                    client_disconnected = True
                    logger.info(f"[SSE {chat_id}] Marking as disconnected after stream_end")
                    
                    # åªåœ¨æ²¡æœ‰å…¶ä»–è¿æ¥æ—¶æ¸…ç†é˜Ÿåˆ—
                    current_connections = active_sse_connections.get(chat_id, 0)
                    logger.info(f"ğŸ”´ [SSE {chat_id}] Current SSE connections after stream_end: {current_connections}")
                    
                    if current_connections <= 1:  # åªæœ‰å½“å‰è¿™ä¸€ä¸ªè¿æ¥æ—¶
                        if chat_id in active_chat_queues:
                            active_chat_queues.pop(chat_id, None)
                            logger.info(f"[SSE {chat_id}] Immediately removed queue from active_chat_queues")
                    else:
                        logger.warning(f"ğŸ”´ [SSE {chat_id}] NOT removing queue - still have {current_connections} connections")
                    
                    break 

                if isinstance(event_data, dict) and "type" in event_data and "data" in event_data:
                    event_type = event_data.get("type", "message")
                    data_payload = event_data.get("data", {})
                    
                    if isinstance(data_payload, str): 
                        formatted_data = data_payload
                    else:
                        try:
                            formatted_data = json.dumps(data_payload)
                        except TypeError:
                            logger.error(f"åºåˆ—åŒ–äº‹ä»¶æ•°æ®ä¸º JSON å¤±è´¥ (type: {event_type}, chat: {chat_id})", exc_info=True)
                            event_type = "error"
                            formatted_data = json.dumps({"message": f"Failed to serialize event data for type {event_type}", "stage": "sse_formatting"})
                    
                    logger.debug(f"SSE Sender for {chat_id}: Sending event type '{event_type}'")
                    yield {
                        "event": event_type,
                        "data": formatted_data
                    }
                else:
                    logger.warning(f"ä»é˜Ÿåˆ—ä¸­è·å–çš„äº‹ä»¶æ ¼å¼ä¸æ­£ç¡® (chat: {chat_id}): {event_data}")
                    yield {
                        "event": "error",
                        "data": json.dumps({"message": "Received malformed event from queue.", "stage": "sse_formatting"})
                    }
                event_queue.task_done()
                await asyncio.sleep(0.01) # Tiny sleep to allow other tasks, prevent tight loop if queue fills fast
        
        except asyncio.CancelledError: # Typically when client disconnects and server cancels the task
            logger.info(f"SSE event sender for chat {chat_id} was explicitly cancelled (outer). Client disconnected: {client_disconnected}")
            # No need to raise, just exit gracefully
        except Exception as e_outer:
            logger.error(f"SSE event sender for chat {chat_id} encountered an UNHANDLED (outer) error: {e_outer}", exc_info=True)
            if not client_disconnected : 
                try:
                    logger.debug(f"[SSE {chat_id}] Attempting to send critical error to client: {e_outer}")
                    yield {
                        "event": "error",
                        "data": json.dumps({"message": f"SSE sender encountered a critical error: {str(e_outer)}", "stage": "sse_sending_critical_outer"})
                    }
                except Exception as send_outer_err:
                    logger.error(f"[SSE {chat_id}] Failed to send final critical (outer) error to client: {send_outer_err}")
        finally:
            # å‡å°‘è¿æ¥è®¡æ•°
            active_sse_connections[chat_id] -= 1
            remaining_connections = active_sse_connections[chat_id]
            logger.info(f"ğŸ”´ SSE event sender for chat {chat_id} is cleaning up. Remaining connections: {remaining_connections}, client_disconnected: {client_disconnected}")
            
            # åªæœ‰å½“æ²¡æœ‰å…¶ä»–è¿æ¥æ—¶æ‰æ¸…ç†é˜Ÿåˆ—
            if remaining_connections == 0:
                logger.info(f"ğŸ”´ No more SSE connections for chat {chat_id}, cleaning up queue")
                active_sse_connections.pop(chat_id, None)  # æ¸…ç†è¿æ¥è®¡æ•°
                
                if chat_id in active_chat_queues and active_chat_queues[chat_id] is event_queue:
                    # Check if queue is empty, if not, log warning as some events might be lost
                    if not event_queue.empty():
                        logger.warning(f"Cleaning up queue for chat {chat_id} but it's not empty. {event_queue.qsize()} items remaining.")
                        # Drain the queue to prevent tasks from hanging on put() if this queue instance is reused (though defaultdict should create new)
                        while not event_queue.empty():
                            try:
                                event_queue.get_nowait()
                                event_queue.task_done()
                            except asyncio.QueueEmpty:
                                break
                    
                    removed_queue = active_chat_queues.pop(chat_id, None)
                    if removed_queue:
                        logger.info(f"å·²æˆåŠŸä» active_chat_queues ä¸­ç§»é™¤ chat {chat_id} çš„é˜Ÿåˆ— (Final cleanup).")
                else:
                    logger.warning(f"åœ¨ SSE æ¸…ç†é˜¶æ®µï¼Œchat {chat_id} çš„é˜Ÿåˆ—å·²ä¸åœ¨ active_chat_queues ä¸­æˆ–ä¸åŒ¹é…å½“å‰å®ä¾‹ï¼Œå¯èƒ½å·²è¢«å…¶ä»–åœ°æ–¹æ¸…ç†ã€‚")
            else:
                logger.warning(f"ğŸ”´ Still have {remaining_connections} SSE connections for chat {chat_id}, NOT cleaning up queue")
            
    return EventSourceResponse(sse_event_sender())


@router.delete("/{chat_id}", response_model=bool)
async def delete_chat(
    chat_id: str,
    db: Session = Depends(get_db), 
    current_user: schemas.User = Depends(get_current_user)
):
    """
    åˆ é™¤èŠå¤©è®°å½•ï¼Œå¿…é¡»ç™»å½•å¹¶ä¸”åªèƒ½åˆ é™¤è‡ªå·±æµç¨‹å›¾çš„èŠå¤©
    """
    chat_service = ChatService(db)
    chat = chat_service.get_chat(chat_id)
    
    if not chat:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="èŠå¤©ä¸å­˜åœ¨"
        )
    
    # éªŒè¯æµç¨‹å›¾å±äºå½“å‰ç”¨æˆ·
    verify_flow_ownership(chat.flow_id, current_user, db)
    
    # åˆ é™¤èŠå¤©
    success = chat_service.delete_chat(chat_id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="åˆ é™¤èŠå¤©å¤±è´¥"
        )
    
    return True 

# --- æå–å‡ºæ¥çš„åå°äº‹ä»¶å¤„ç†å‡½æ•° ---
async def _process_and_publish_chat_events(
    chat_id: str, 
    initial_user_message_content: Optional[str], 
    event_queue: asyncio.Queue,
    is_edit_flow: bool = False,
    client_message_id: Optional[str] = None
):
    """
    åå°ä»»åŠ¡ï¼šå¤„ç†èŠå¤©é€»è¾‘ï¼ˆæ·»åŠ æ¶ˆæ¯ï¼Œè°ƒç”¨LangGraphï¼‰ï¼Œå¹¶é€šè¿‡é˜Ÿåˆ—å‘å¸ƒSSEäº‹ä»¶ã€‚
    """
    logger.debug(f"[Chat {chat_id}] Background task started (is_edit_flow: {is_edit_flow}). Initial content (if any): {initial_user_message_content}")
    is_error = False
    error_data = {}
    final_reply_accumulator = ""
    final_state = None
    # Declare token_cv here to ensure it's in scope for the finally block
    token_cv = None 

    try:
        with get_db_context() as db_session_bg:
            logger.debug(f"[Chat {chat_id}] Acquired DB session for background task.")
            chat_service_bg = ChatService(db_session_bg)
            flow_service_bg = FlowService(db_session_bg)

            chat = chat_service_bg.get_chat(chat_id)
            if not chat:
                logger.error(f"[Chat {chat_id}] Background task could not find chat.")
                await event_queue.put({"type": "error", "data": {"message": "Chat not found.", "stage": "setup"}})
                return
            
            # If it's a new message flow (not an edit), add the user message to DB.
            # For an edit flow, edit_user_message_and_truncate already updated the DB.
            if not is_edit_flow and initial_user_message_content is not None:
                logger.debug(f"[Chat {chat_id}] Attempting to save user message to DB before agent call: {initial_user_message_content[:100]}...")
                # chat_service_bg.add_message_to_chat è¿”å› (Chat, str) æˆ– (None, None)
                saved_chat_obj, server_message_timestamp = chat_service_bg.add_message_to_chat(
                    chat_id=chat_id, 
                    role="user", 
                    content=initial_user_message_content
                )
                if saved_chat_obj and server_message_timestamp:
                    logger.debug(f"[Chat {chat_id}] User message saved to DB with server_timestamp: {server_message_timestamp}.")
                    # å¦‚æœæœ‰ client_message_idï¼Œåˆ™æ¨é€äº‹ä»¶å‘ŠçŸ¥å‰ç«¯æ—¶é—´æˆ³å¯¹åº”å…³ç³»
                    if client_message_id:
                        await event_queue.put({
                            "type": "user_message_saved", 
                            "data": {
                                "client_message_id": client_message_id,
                                "server_message_timestamp": server_message_timestamp,
                                "content": initial_user_message_content # å¯ä»¥é€‰æ‹©æ€§åŒ…å«å†…å®¹ä»¥ä¾›å‰ç«¯æ ¡éªŒ
                            }
                        })
                        logger.info(f"[Chat {chat_id}] Sent user_message_saved event for client_id: {client_message_id} -> server_ts: {server_message_timestamp}")
                    
                    # Re-fetch chat to ensure chat_data is up-to-date for history formatting
                    # ä½¿ç”¨è¿”å›çš„ saved_chat_obj å³å¯ï¼Œæ— éœ€é‡æ–°æŸ¥è¯¢
                    chat = saved_chat_obj 
                else:
                    logger.error(f"[Chat {chat_id}] Failed to save user message to DB.")
                    await event_queue.put({
                        "type": "error", 
                        "data": {"message": "Failed to save user message.", "stage": "setup"}
                    })
                    return # å¦‚æœæ¶ˆæ¯ä¿å­˜å¤±è´¥ï¼Œåˆ™ç»ˆæ­¢åç»­å¤„ç†


            flow_id = chat.flow_id
            token_cv = current_flow_id_var.set(flow_id) 
            logger.debug(f"[Chat {chat_id}] Set current_flow_id_var to {flow_id}")

            flow = flow_service_bg.get_flow_instance(flow_id)
            if not flow:
                logger.error(f"[Chat {chat_id}] Background task could not find flow {flow_id}.")
                await event_queue.put({"type": "error", "data": {"message": f"Flow {flow_id} not found.", "stage": "setup"}})
                current_flow_id_var.reset(token_cv)
                return
            
            flow_data = flow.flow_data or {}
            logger.debug(f"[Chat {chat_id}] Flow data for context: {str(flow_data)[:200]}...")

            logger.debug(f"[Chat {chat_id}] Getting compiled LangGraph from ChatService.")
            compiled_graph = chat_service_bg.compiled_workflow_graph
            logger.debug(f"[Chat {chat_id}] Successfully got compiled LangGraph.")

            chat_history_raw = chat.chat_data.get("messages", [])
            
            # The graph input should be ALL messages from history.
            # The last message in chat_history_raw is the one the agent needs to respond to.
            graph_input_messages = _format_messages_to_langchain(chat_history_raw)
            
            # Determine the current user input based on the last message in the formatted history
            current_user_input_content = ""
            if graph_input_messages and isinstance(graph_input_messages[-1], HumanMessage):
                current_user_input_content = graph_input_messages[-1].content
            else: # Should not happen if history is well-formed and ends with a user message
                logger.warning(f"[Chat {chat_id}] Could not determine current user input from chat history. Last message: {graph_input_messages[-1] if graph_input_messages else 'No messages'}")
                # Fallback, though this indicates an issue upstream (e.g. after edit, no user message is last)
                if initial_user_message_content and not is_edit_flow: # Use initial content if new message
                     current_user_input_content = initial_user_message_content
                elif is_edit_flow and chat_history_raw: # If edit, try to get last message from raw data
                    last_raw_msg = chat_history_raw[-1]
                    if last_raw_msg.get("role") == "user":
                        current_user_input_content = last_raw_msg.get("content", "")


            graph_input = {
                "messages": graph_input_messages, # Full history including the latest user message
                "input": current_user_input_content, # The content of the latest user message
                "flow_context": flow_data.get("graphContextVars", {}),
                "current_flow_id": flow_id,
            }
            
            messages_count_val = len(graph_input["messages"])
            input_len_val = len(graph_input["input"])
            flow_id_val = graph_input["current_flow_id"]
            logger.debug(f"[Chat {chat_id}] Prepared graph input: messages_count={messages_count_val}, input_len={input_len_val}, input_content='{current_user_input_content[:50]}...', flow_id={flow_id_val}")

            logger.debug(f"[Chat {chat_id}] Invoking compiled_graph.astream_events (version='v2')...")
            
            event_include_names = None

            async for event in compiled_graph.astream_events(graph_input, version="v2", include_names=event_include_names, include_tags=None):
                event_name = event.get("event")
                event_data = event.get("data", {})
                run_name = event.get("name", "unknown_run")

                # æ·»åŠ æ›´è¯¦ç»†çš„äº‹ä»¶æ—¥å¿—
                logger.info(f"[Chat {chat_id}] ğŸ” Received event: '{event_name}' from '{run_name}', Data keys: {list(event_data.keys())}")
                
                # ç‰¹åˆ«å…³æ³¨Chain Endäº‹ä»¶
                if event_name == "on_chain_end":
                    logger.info(f"[Chat {chat_id}] ğŸš¨ CHAIN END DETECTED: run_name='{run_name}', compiled_graph.name='{compiled_graph.name}'")
                    logger.info(f"[Chat {chat_id}] ğŸš¨ Output data type: {type(event_data.get('output', 'NO_OUTPUT'))}")
                    if isinstance(event_data.get('output'), dict):
                        output_keys = list(event_data.get('output', {}).keys())
                        logger.info(f"[Chat {chat_id}] ğŸš¨ Output keys: {output_keys}")
                
                # è®°å½•æ‰€æœ‰ä¸åŒç±»å‹çš„äº‹ä»¶
                if event_name not in ["on_chat_model_stream"]:  # é¿å…tokenæµçš„æ—¥å¿—è¿‡å¤š
                    logger.info(f"[Chat {chat_id}] ğŸ“‹ Event details - Name: {event_name}, Run: {run_name}, Data type: {type(event_data)}")

                if event_name == "on_chat_model_stream":
                    chunk = event_data.get("chunk")
                    if chunk and isinstance(chunk, AIMessageChunk) and chunk.content:
                        token = chunk.content
                        logger.debug(f"[Chat {chat_id}] LLM Token from '{run_name}': '{token}'")
                        await event_queue.put({"type": "token", "data": token})
                        final_reply_accumulator += token 
                    elif chunk:
                        logger.debug(f"[Chat {chat_id}] Received on_chat_model_stream chunk from '{run_name}' but no content or not AIMessageChunk. Chunk: {chunk}")

                elif event_name == "on_llm_end":
                    output = event_data.get("output")
                    if output and isinstance(output, AIMessage) and output.content:
                        logger.info(f"[Chat {chat_id}] LLM End from '{run_name}'. Full output (for verification): '{output.content[:100]}...'")
                        # This check might be too strict if streaming involves minor post-processing.
                        # Consider if the primary source of truth for final_reply_accumulator should be this on_llm_end if available.
                        # For now, it just logs a warning.
                        if final_reply_accumulator != output.content and not final_reply_accumulator.endswith(output.content):
                            logger.warning(f"[Chat {chat_id}] Discrepancy between accumulated stream and on_llm_end output from '{run_name}'. Accum: '{final_reply_accumulator[:100]}...', Output: '{output.content[:100]}...'")
                    elif output:
                         logger.debug(f"[Chat {chat_id}] Received on_llm_end from '{run_name}' but no content or not AIMessage. Output: {output}")

                elif event_name == "on_tool_start":
                    tool_name = event_data.get("name")
                    tool_input = event_data.get("input")
                    logger.info(f"[Chat {chat_id}] Tool Start: '{tool_name}' from '{run_name}' with input: {str(tool_input)[:100]}...")
                    await event_queue.put({"type": "tool_start", "data": {"name": tool_name, "input": tool_input}})
                    
                elif event_name == "on_tool_end":
                    tool_name = event_data.get("name")
                    tool_output = event_data.get("output")
                    output_summary = str(tool_output)
                    if len(output_summary) > 200:
                        output_summary = output_summary[:200] + "..."
                    logger.info(f"[Chat {chat_id}] Tool End: '{tool_name}' from '{run_name}' with output: {output_summary}")
                    await event_queue.put({"type": "tool_end", "data": {"name": tool_name, "output_summary": output_summary, "full_output": tool_output}})
                    
                    # æ£€æŸ¥ç‰¹å®šå·¥å…·æ˜¯å¦éœ€è¦è§¦å‘çŠ¶æ€åŒæ­¥
                    if tool_name and "sas" in tool_name.lower() and isinstance(tool_output, dict):
                        # æ£€æŸ¥å·¥å…·è¾“å‡ºæ˜¯å¦åŒ…å«é‡è¦çŠ¶æ€
                        important_keys = ['sas_step1_generated_tasks', 'sas_step2_generated_task_details', 'dialog_state']
                        if any(key in tool_output for key in important_keys):
                            logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' è¾“å‡ºåŒ…å«é‡è¦çŠ¶æ€ï¼Œè§¦å‘åŒæ­¥")
                            sync_result = _sync_langgraph_state_to_flow(tool_output, flow_id, flow_service_bg)
                            
                            # å¦‚æœåŒæ­¥æˆåŠŸä¸”éœ€è¦å‰ç«¯æ›´æ–°ï¼Œå‘é€é€šçŸ¥äº‹ä»¶
                            if sync_result and sync_result.get("needs_frontend_update"):
                                logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…·ç»“æŸåå‘é€agent_state_updatedäº‹ä»¶åˆ°å‰ç«¯")
                                await event_queue.put({
                                    "type": "agent_state_updated", 
                                    "data": {
                                        "message": f"Agent state updated by tool '{tool_name}'",
                                        "update_types": sync_result.get("update_types", []),
                                        "flow_id": flow_id,
                                        "agent_state": sync_result.get("updated_agent_state", {}),
                                        "trigger": "tool_end"
                                    }
                                })
                            else:
                                logger.warning(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' åŒæ­¥æœªäº§ç”Ÿå‰ç«¯æ›´æ–°éœ€æ±‚")
                        else:
                            logger.info(f"[Chat {chat_id}] ğŸ¯ å·¥å…· '{tool_name}' è¾“å‡ºä¸åŒ…å«é‡è¦çŠ¶æ€é”®: {list(tool_output.keys())}")

                elif event_name == "on_chain_end":
                    outputs_from_chain = event_data.get("output", {})
                    logger.info(f"[Chat {chat_id}] ğŸš¨ Chain End: '{run_name}'. Output keys: {list(outputs_from_chain.keys()) if isinstance(outputs_from_chain, dict) else 'Not a dict'}")
                    logger.info(f"[Chat {chat_id}] ğŸš¨ Chain End output type: {type(outputs_from_chain)}")
                    logger.info(f"[Chat {chat_id}] ğŸš¨ Chain End output content: {str(outputs_from_chain)[:500]}...")
                    
                    # å¤šç§åŒæ­¥è§¦å‘æ¡ä»¶
                    should_sync = False
                    sync_reason = ""
                    
                    # 1. ä¸»å›¾ç»“æŸæ—¶åŒæ­¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                    if run_name == compiled_graph.name or run_name == "__graph__":
                        should_sync = True
                        sync_reason = "ä¸»å›¾æ‰§è¡Œç»“æŸ"
                        final_state = outputs_from_chain
                        logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶1: ä¸»å›¾ç»“æŸ (run_name: {run_name}, graph_name: {compiled_graph.name})")
                        
                    # 2. SASå­å›¾é‡è¦èŠ‚ç‚¹æ‰§è¡Œå®Œæˆæ—¶åŒæ­¥
                    elif "sas" in run_name.lower() and isinstance(outputs_from_chain, dict):
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«éœ€è¦åŒæ­¥çš„é‡è¦çŠ¶æ€
                        important_keys = [
                            'sas_step1_generated_tasks',
                            'sas_step2_generated_task_details', 
                            'dialog_state',
                            'task_list_accepted',
                            'module_steps_accepted',
                            'current_user_request'
                        ]
                        
                        found_keys = [key for key in important_keys if key in outputs_from_chain]
                        if found_keys:
                            should_sync = True
                            sync_reason = f"SASå­å›¾çŠ¶æ€æ›´æ–° (run_name: {run_name}, found_keys: {found_keys})"
                            final_state = outputs_from_chain
                            logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶2: SASå­å›¾çŠ¶æ€æ›´æ–°")
                            logger.info(f"[Chat {chat_id}] ğŸ¯ å‘ç°é‡è¦é”®: {found_keys}")
                        else:
                            logger.info(f"[Chat {chat_id}] ğŸ¯ SASå­å›¾ç»“æŸä½†æ— é‡è¦çŠ¶æ€: {run_name}, keys: {list(outputs_from_chain.keys())}")
                    
                    # 3. æœºå™¨äººæµç¨‹è°ƒç”¨èŠ‚ç‚¹å®Œæˆæ—¶åŒæ­¥
                    elif "robot_flow_invoker" in run_name.lower() and isinstance(outputs_from_chain, dict):
                        if "sas_planner_subgraph_state" in outputs_from_chain:
                            should_sync = True
                            sync_reason = f"æœºå™¨äººæµç¨‹èŠ‚ç‚¹å®Œæˆ (run_name: {run_name})"
                            final_state = outputs_from_chain
                            logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘æ¡ä»¶3: æœºå™¨äººæµç¨‹èŠ‚ç‚¹å®Œæˆ")
                        else:
                            logger.info(f"[Chat {chat_id}] ğŸ¯ æœºå™¨äººæµç¨‹èŠ‚ç‚¹ç»“æŸä½†æ— å­å›¾çŠ¶æ€: {run_name}")
                    
                    # è®°å½•æœªè§¦å‘åŒæ­¥çš„æƒ…å†µ
                    if not should_sync:
                        logger.info(f"[Chat {chat_id}] ğŸ¯ Chain Endä¸æ»¡è¶³åŒæ­¥æ¡ä»¶: run_name='{run_name}', graph_name='{compiled_graph.name}', is_dict={isinstance(outputs_from_chain, dict)}")
                    
                    # æ‰§è¡ŒåŒæ­¥
                    if should_sync:
                        logger.info(f"[Chat {chat_id}] ğŸ¯ è§¦å‘åŒæ­¥ - åŸå› : {sync_reason}")
                        logger.info(f"[Chat {chat_id}] ğŸ¯ Final state keys: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not a dict'}. Content: {str(final_state)[:500]}...")
                        
                        if isinstance(final_state, dict):
                            sync_result = _sync_langgraph_state_to_flow(final_state, flow_id, flow_service_bg)
                            
                            # å¦‚æœåŒæ­¥æˆåŠŸä¸”éœ€è¦å‰ç«¯æ›´æ–°ï¼Œå‘é€é€šçŸ¥äº‹ä»¶
                            if sync_result and sync_result.get("needs_frontend_update"):
                                logger.info(f"[Chat {chat_id}] ğŸ¯ å‘é€agent_state_updatedäº‹ä»¶åˆ°å‰ç«¯")
                                await event_queue.put({
                                    "type": "agent_state_updated", 
                                    "data": {
                                        "message": "Agent state has been updated with new tasks/details",
                                        "update_types": sync_result.get("update_types", []),
                                        "flow_id": flow_id,
                                        "agent_state": sync_result.get("updated_agent_state", {})
                                    }
                                })
                            else:
                                logger.warning(f"[Chat {chat_id}] ğŸ¯ åŒæ­¥å®Œæˆä½†æ— éœ€å‰ç«¯æ›´æ–°: sync_result={sync_result}")
                        else:
                            logger.warning(f"[Chat {chat_id}] ğŸ¯ final_stateä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè·³è¿‡åŒæ­¥ã€‚ç±»å‹: {type(final_state)}")
                    
                    # ä¸»å›¾ç»“æŸæ—¶çš„ç‰¹æ®Šå¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                    if run_name == compiled_graph.name or run_name == "__graph__":

                        latest_ai_message_from_state: Optional[str] = None
                        if isinstance(final_state, dict) and "messages" in final_state and isinstance(final_state["messages"], list):
                            
                            for msg_state in reversed(final_state["messages"]): # Original loop
                                content_candidate: Optional[str] = None
                                is_ai_message = False

                                if isinstance(msg_state, AIMessage):
                                    is_ai_message = True
                                    if hasattr(msg_state, 'content'):
                                        content_candidate = msg_state.content
                                elif isinstance(msg_state, dict) and msg_state.get("type") == "ai":
                                    is_ai_message = True
                                    content_candidate = msg_state.get("content")
                                
                                if is_ai_message:
                                    if content_candidate is not None and isinstance(content_candidate, str) and content_candidate.strip():
                                        latest_ai_message_from_state = content_candidate
                                        logger.info(f"[Chat {chat_id}] Found latest AI message (type: {type(msg_state)}) in final graph state: '{latest_ai_message_from_state[:100]}...'")
                                    else:
                                        logger.info(f"[Chat {chat_id}] Found AI-like message (type: {type(msg_state)}) in final state, but content is None, not string, or empty/whitespace. Original content: '{str(content_candidate)[:100]}...'")
                                    break # Found the latest AI-like message, break from loop
                                # If not an AI message, continue to the next older message
                        
                        if latest_ai_message_from_state:
                            if not final_reply_accumulator:
                                logger.info(f"[Chat {chat_id}] No prior stream. Using AIMessage from final graph state as the reply: '{latest_ai_message_from_state[:100]}...'")
                                final_reply_accumulator = latest_ai_message_from_state
                                await event_queue.put({"type": "token", "data": latest_ai_message_from_state})
                            elif final_reply_accumulator != latest_ai_message_from_state:
                                logger.warning(f"[Chat {chat_id}] Accumulated stream reply ('{final_reply_accumulator[:100]}...') differs from final graph state AIMessage ('{latest_ai_message_from_state[:100]}...').")
                                logger.info(f"[Chat {chat_id}] Overwriting accumulated stream with final AIMessage from graph state for frontend and saving.")
                                final_reply_accumulator = latest_ai_message_from_state
                                # Send this authoritative message. If frontend simply appends tokens, this might lead to duplication
                                # or mixed messages if not handled carefully by client.
                                # A more robust solution might involve a special event type e.g., "final_message" or "replace_content".
                                # For now, sending as "token" to ensure it's displayed.
                                await event_queue.put({"type": "token", "data": latest_ai_message_from_state})
                            # If final_reply_accumulator == latest_ai_message_from_state, it means stream matched final state, no action needed.
                        else: # No valid AIMessage found in final_state
                            if final_reply_accumulator:
                                logger.info(f"[Chat {chat_id}] Graph ended. No new AIMessage in final state. Using previously accumulated stream as final reply: '{final_reply_accumulator[:100]}...'")
                            else:
                                logger.warning(f"[Chat {chat_id}] Graph ended. No AIMessage in final state and no accumulated stream. Reply will be empty/null if no default is set later.")
                                # final_reply_accumulator remains empty or None.
                
                elif event_name == "on_chain_error" or event_name == "on_llm_error" or event_name == "on_tool_error":
                    error_content = str(event_data.get("error", "Unknown error"))
                    logger.error(f"[Chat {chat_id}] Error event '{event_name}' from '{run_name}': {error_content}")
                    is_error = True
                    stage = f"error_in_{run_name}"
                    error_obj = event_data.get("error")
                    specific_error_message = str(error_obj) if error_obj else "Details not available"
                    
                    error_data = {"message": f"Error in {run_name}: {specific_error_message}", "stage": stage, "details": str(error_obj)}
                    await event_queue.put({"type": "error", "data": error_data})

    except Exception as e:
        is_error = True
        error_message = f"Error during LangGraph astream_events processing: {str(e)}"
        logger.error(f"[Chat {chat_id}] {error_message}", exc_info=True)
        error_data = {"message": error_message, "stage": "graph_execution"}
        try:
            await event_queue.put({"type": "error", "data": error_data})
        except asyncio.QueueFull:
            logger.error(f"[Chat {chat_id}] Failed to put error message in full queue after main exception.")
        except Exception as qe:
            logger.error(f"[Chat {chat_id}] Failed to put error message in queue after main exception: {qe}")

    finally:
        if token_cv is not None: 
            current_flow_id_var.reset(token_cv)
            logger.debug(f"[Chat {chat_id}] Reset current_flow_id context variable in finally block.")
        else:
            logger.debug(f"[Chat {chat_id}] current_flow_id_var might not have been set or was already reset, skipping reset in finally.")

        # --- æ–°å¢ï¼šå¤„ç†ä¼šè¯ç»“æŸæ—¶çš„é»˜è®¤å›å¤ ---
        session_should_end = False
        if isinstance(final_state, dict) and final_state.get("output") == "__end__":
            session_should_end = True
        elif isinstance(final_state, str) and final_state == "__end__": # Fallback for simpler __end__ signal
            session_should_end = True
        # You might have other ways to check if the session should end based on your specific final_state structure
        # For example, if final_state has a specific key from your graph like final_state.get('next_node') == '__end__'

        if session_should_end and not final_reply_accumulator and not is_error:
            default_goodbye_message = "å¥½çš„ï¼Œå†è§ï¼å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥å†æ¬¡è”ç³»æˆ‘ã€‚"
            logger.info(f"[Chat {chat_id}] Session is ending and no AI reply was generated. Using default goodbye: '{default_goodbye_message}'")
            final_reply_accumulator = default_goodbye_message
            try:
                # Ensure this default message is also sent as a token to the client
                await event_queue.put({"type": "token", "data": default_goodbye_message})
                logger.debug(f"[Chat {chat_id}] Sent default goodbye message to event queue.")
            except asyncio.QueueFull:
                logger.error(f"[Chat {chat_id}] Failed to put default goodbye message in full queue.")
            except Exception as qe_goodbye:
                logger.error(f"[Chat {chat_id}] Failed to put default goodbye message in queue: {qe_goodbye}")
        # --- ç»“æŸæ–°å¢ ---

        if not is_error and final_reply_accumulator:
            try:
                with get_db_context() as db_session_final:
                    chat_service_final = ChatService(db_session_final)
                    logger.info(f"[Chat {chat_id}] Saving AI assistant reply to DB: {final_reply_accumulator[:100]}...")
                    chat_service_final.add_message_to_chat(
                        chat_id=chat_id,
                        role="assistant",
                        content=final_reply_accumulator
                    )
                    logger.info(f"[Chat {chat_id}] AI assistant reply saved to DB successfully.")
            except Exception as save_err:
                logger.error(f"[Chat {chat_id}] Failed to save AI reply to DB: {save_err}", exc_info=True)
        elif is_error:
            logger.warning(f"[Chat {chat_id}] Skipping AI reply save due to an error during processing. Error: {error_data}")
        else:
            logger.warning(f"[Chat {chat_id}] Skipping save because final reply was empty or null. Accumulator content: '{final_reply_accumulator}'")
        
        try:
            logger.debug(f"[Chat {chat_id}] Putting STREAM_END_SENTINEL into queue.")
            await event_queue.put(STREAM_END_SENTINEL)
            logger.debug(f"[Chat {chat_id}] Stream end sentinel sent.")
        except asyncio.QueueFull:
            logger.error(f"[Chat {chat_id}] Failed to put STREAM_END_SENTINEL in full queue.")
        except Exception as qe:
            logger.error(f"[Chat {chat_id}] Failed to put STREAM_END_SENTINEL in queue: {qe}")
        
        # --- æ–°å¢ï¼šç¡®ä¿å‘æ‰€æœ‰å¯èƒ½çš„é˜Ÿåˆ—å‘é€STREAM_END_SENTINEL ---
        # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„é˜Ÿåˆ—ï¼Œå¦‚æœæœ‰ï¼Œç¡®ä¿å‘é€ç»“æŸä¿¡å·
        if chat_id in active_chat_queues and active_chat_queues[chat_id] is not event_queue:
            try:
                current_queue = active_chat_queues[chat_id]
                logger.info(f"[Chat {chat_id}] Found active queue during cleanup. Queue size: {current_queue.qsize()}")
                
                # æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦å·²ç»æœ‰STREAM_END_SENTINEL
                has_end_sentinel = False
                temp_items = []
                while not current_queue.empty():
                    try:
                        item = current_queue.get_nowait()
                        temp_items.append(item)
                        if item is STREAM_END_SENTINEL:
                            has_end_sentinel = True
                            logger.info(f"[Chat {chat_id}] Found existing STREAM_END_SENTINEL in queue")
                        current_queue.task_done()
                    except asyncio.QueueEmpty:
                        break
                
                # å°†ä¸´æ—¶å–å‡ºçš„é¡¹ç›®æ”¾å›é˜Ÿåˆ—
                for item in temp_items:
                    await current_queue.put(item)
                
                # å¦‚æœæ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œæ·»åŠ ä¸€ä¸ª
                if not has_end_sentinel:
                    logger.info(f"[Chat {chat_id}] No STREAM_END_SENTINEL found in queue, adding one now")
                    await current_queue.put(STREAM_END_SENTINEL)
                    logger.info(f"[Chat {chat_id}] STREAM_END_SENTINEL added to queue during cleanup")
                else:
                    logger.info(f"[Chat {chat_id}] STREAM_END_SENTINEL already exists in queue, skipping duplicate")
                    
            except asyncio.QueueFull:
                logger.error(f"[Chat {chat_id}] Failed to add STREAM_END_SENTINEL during cleanup - queue is full")
            except Exception as cleanup_err:
                logger.error(f"[Chat {chat_id}] Error during queue cleanup: {cleanup_err}", exc_info=True)
        else:
            logger.info(f"[Chat {chat_id}] No active queue found during cleanup")
            
        logger.info(f"[Chat {chat_id}] Background task (is_edit_flow: {is_edit_flow}) final cleanup completed.")

def _sync_langgraph_state_to_flow(final_state, flow_id, flow_service_bg):
    """
    å°†LangGraphæ‰§è¡Œçš„final_stateåŒæ­¥åˆ°ä¸»Flowçš„agent_state
    æ”¯æŒå¤šç§é‡è¦çŠ¶æ€çš„åŒæ­¥ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç”Ÿæˆã€è¯¦æƒ…ç”Ÿæˆç­‰
    **æ–°å¢**ï¼šæ”¯æŒä»sas_planner_subgraph_stateä¸­æå–SASå­å›¾æ•°æ®
    """
    try:
        logger.info(f"[Flow {flow_id}] ğŸ¯ å¼€å§‹åŒæ­¥LangGraphçŠ¶æ€åˆ°Flow agent_state...")
        logger.info(f"[Flow {flow_id}] ğŸ¯ final_stateé”®å€¼: {list(final_state.keys()) if isinstance(final_state, dict) else 'Not a dict'}")
        logger.info(f"[Flow {flow_id}] ğŸ¯ final_stateå†…å®¹æ‘˜è¦: {str(final_state)[:1000]}...")
        
        # è·å–å½“å‰Flowçš„agent_state
        flow = flow_service_bg.get_flow_instance(flow_id)
        if not flow:
            logger.error(f"[Flow {flow_id}] æ— æ³•æ‰¾åˆ°Flowï¼ŒåŒæ­¥å¤±è´¥")
            return None
        
        current_agent_state = flow.agent_state or {}
        logger.info(f"[Flow {flow_id}] ğŸ¯ å½“å‰agent_stateé”®å€¼: {list(current_agent_state.keys())}")
        
        # æ£€æŸ¥éœ€è¦åŒæ­¥çš„çŠ¶æ€å˜åŒ–
        needs_sync = False
        sync_updates = {}
        update_types = []
        
        # 1. æ£€æŸ¥ä¸»å›¾çŠ¶æ€ä¸­çš„ç›´æ¥å­—æ®µ
        important_fields = [
            'sas_step1_generated_tasks',
            'sas_step2_generated_task_details', 
            'task_list_accepted',
            'module_steps_accepted',
            'dialog_state',
            'subgraph_completion_status',
            'current_user_request',
            'revision_iteration',
            'input_processed'
        ]
        
        for field in important_fields:
            if field in final_state:
                current_value = current_agent_state.get(field)
                new_value = final_state[field]
                if current_value != new_value:
                    logger.info(f"[Flow {flow_id}] ğŸ¯ æ£€æµ‹åˆ°{field}å˜åŒ–: {current_value} -> {new_value}")
                    sync_updates[field] = new_value
                    update_types.append(field)
                    needs_sync = True
        
        # 2. **æ–°å¢**ï¼šæ£€æŸ¥sas_planner_subgraph_stateï¼ˆå…³é”®ä¿®å¤ï¼‰
        sas_subgraph_state = final_state.get('sas_planner_subgraph_state')
        if sas_subgraph_state and isinstance(sas_subgraph_state, dict):
            logger.info(f"[Flow {flow_id}] ğŸ¯ å‘ç°SASå­å›¾çŠ¶æ€ï¼Œå¼€å§‹æå–æ•°æ®...")
            logger.info(f"[Flow {flow_id}] ğŸ¯ SASå­å›¾çŠ¶æ€é”®å€¼: {list(sas_subgraph_state.keys())}")
            
            # æå–SASå­å›¾ä¸­çš„é‡è¦æ•°æ®
            sas_important_fields = [
                'sas_step1_generated_tasks',
                'sas_step2_generated_task_details',
                'sas_step2_module_steps',
                'task_list_accepted',
                'module_steps_accepted',
                'dialog_state',
                'subgraph_completion_status',
                'current_user_request',
                'revision_iteration'
            ]
            
            for field in sas_important_fields:
                if field in sas_subgraph_state:
                    current_value = current_agent_state.get(field)
                    new_value = sas_subgraph_state[field]
                    if current_value != new_value:
                        logger.info(f"[Flow {flow_id}] ğŸ¯ ä»SASå­å›¾æ£€æµ‹åˆ°{field}å˜åŒ–: {current_value} -> {new_value}")
                        sync_updates[field] = new_value
                        update_types.append(f"sas_{field}")
                        needs_sync = True
            
            # ç‰¹åˆ«å¤„ç†ä»»åŠ¡æ•°æ®çš„è¯¦ç»†æ£€æŸ¥
            sas_tasks = sas_subgraph_state.get('sas_step1_generated_tasks')
            if sas_tasks:
                logger.info(f"[Flow {flow_id}] ğŸ¯ SASå­å›¾åŒ…å« {len(sas_tasks)} ä¸ªä»»åŠ¡:")
                for i, task in enumerate(sas_tasks):
                    if isinstance(task, dict):
                        logger.info(f"[Flow {flow_id}] ğŸ¯   ä»»åŠ¡{i+1}: {task.get('name', 'Unknown')} (ç±»å‹: {task.get('type', 'Unknown')})")
                    else:
                        logger.info(f"[Flow {flow_id}] ğŸ¯   ä»»åŠ¡{i+1}: {task}")
            
            sas_details = sas_subgraph_state.get('sas_step2_generated_task_details')
            if sas_details:
                logger.info(f"[Flow {flow_id}] ğŸ¯ SASå­å›¾åŒ…å«ä»»åŠ¡è¯¦æƒ…: {len(sas_details)} é¡¹")
                for task_key, details in sas_details.items():
                    if isinstance(details, dict) and 'details' in details:
                        detail_count = len(details['details']) if isinstance(details['details'], list) else 1
                        logger.info(f"[Flow {flow_id}] ğŸ¯   {task_key}: {detail_count} ä¸ªè¯¦æƒ…")
        
        # 3. å¼ºåˆ¶æ£€æŸ¥æ˜¯å¦éœ€è¦å‰ç«¯èŠ‚ç‚¹æ›´æ–°ï¼ˆå…³é”®é€»è¾‘ï¼‰
        has_task_data = (
            sync_updates.get('sas_step1_generated_tasks') or 
            current_agent_state.get('sas_step1_generated_tasks') or
            (sas_subgraph_state and sas_subgraph_state.get('sas_step1_generated_tasks'))
        )
        
        has_detail_data = (
            sync_updates.get('sas_step2_generated_task_details') or
            current_agent_state.get('sas_step2_generated_task_details') or
            (sas_subgraph_state and sas_subgraph_state.get('sas_step2_generated_task_details'))
        )
        
        needs_frontend_update = has_task_data or has_detail_data
        if needs_frontend_update:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ£€æµ‹åˆ°ä»»åŠ¡æ•°æ®ï¼Œéœ€è¦å‰ç«¯èŠ‚ç‚¹æ›´æ–°")
            update_types.append("frontend_nodes")
            needs_sync = True
        
        # 4. æ‰§è¡ŒåŒæ­¥æ›´æ–°
        if needs_sync:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ‰§è¡ŒçŠ¶æ€åŒæ­¥ï¼Œæ›´æ–° {len(sync_updates)} ä¸ªå­—æ®µ:")
            for key, value in sync_updates.items():
                logger.info(f"[Flow {flow_id}] ğŸ¯   {key}: {str(value)[:200]}...")
            
            # æ›´æ–°Flowçš„agent_state
            current_agent_state.update(sync_updates)
            flow.agent_state = current_agent_state
            
            # å‡†å¤‡è¿”å›ç»“æœ
            result = {
                "needs_frontend_update": needs_frontend_update,
                "update_types": update_types,
                "updated_agent_state": current_agent_state,
                "sync_updates": sync_updates
            }
            
            logger.info(f"[Flow {flow_id}] ğŸ¯ çŠ¶æ€åŒæ­¥å®Œæˆï¼Œéœ€è¦å‰ç«¯æ›´æ–°: {needs_frontend_update}")
            return result
        else:
            logger.info(f"[Flow {flow_id}] ğŸ¯ æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦åŒæ­¥çš„çŠ¶æ€å˜åŒ–")
            return None
    
    except Exception as e:
        logger.error(f"[Flow {flow_id}] ğŸ¯ çŠ¶æ€åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return None 